{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpiBgmoviSjC76fJ0LbdDa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carloscotrini/transformers_from_scratch/blob/main/AML_MyTransfomerV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GWhXikQiMT2",
        "outputId": "491a9c07-472c-43d0-b5e0-a5d1049e8bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a circle then triangle after triangle\n",
            "one circle and two triangles\n",
            "a triangle then a triangle\n",
            "a circle\n",
            "a circle\n",
            "circle after triangle then one triangle\n",
            "one triangle and one circle\n",
            "two triangles\n",
            "a triangle\n",
            "one circle\n",
            "one triangle then triangle after triangle\n",
            "one triangle\n",
            "triangle after triangle then one triangle\n",
            "a triangle\n",
            "a triangle then triangle after circle and one circle\n",
            "two triangles\n",
            "triangle after triangle\n",
            "circle after circle then a circle\n",
            "triangle after triangle then one triangle then a circle\n",
            "circle after triangle and one triangle\n",
            "one circle and triangle after triangle and a circle\n",
            "one triangle and circle after circle then one circle\n",
            "circle after triangle then one triangle\n",
            "a circle\n",
            "circle after circle and a circle\n",
            "triangle after triangle and triangle after triangle\n",
            "a triangle then a circle\n",
            "triangle after circle and triangle after circle\n",
            "a circle\n",
            "a triangle\n",
            "triangle after circle and a triangle then a triangle\n",
            "circle after triangle\n",
            "a circle\n",
            "one circle and two triangles\n",
            "a circle\n",
            "triangle after triangle\n",
            "one circle\n",
            "two triangles\n",
            "triangle after circle then triangle after triangle\n",
            "one triangle then one triangle and two circles\n",
            "a triangle\n",
            "two triangles\n",
            "two triangles\n",
            "a triangle then one triangle\n",
            "triangle after circle then a triangle then a circle\n",
            "circle after circle\n",
            "circle after circle then one triangle\n",
            "a triangle and a circle then one triangle\n",
            "a triangle then two triangles\n",
            "one circle and one circle and one triangle and one circle\n",
            "two triangles\n",
            "a triangle and one circle then triangle after circle\n",
            "circle after circle then a triangle and one triangle\n",
            "two circles\n",
            "a triangle\n",
            "triangle after circle and one circle and one triangle\n",
            "a circle\n",
            "one circle\n",
            "a triangle then a circle and one triangle then a circle\n",
            "one circle\n",
            "circle after triangle and a triangle and a triangle\n",
            "triangle after circle then a circle\n",
            "triangle after circle then triangle after circle\n",
            "circle after circle and circle after triangle\n",
            "triangle after circle\n",
            "circle after circle and one triangle\n",
            "a triangle then circle after triangle\n",
            "triangle after circle\n",
            "a circle\n",
            "one triangle then a triangle\n",
            "circle after triangle and one circle\n",
            "triangle after circle\n",
            "two circles\n",
            "a circle and a circle then a triangle then one circle\n",
            "triangle after triangle and triangle after triangle\n",
            "triangle after triangle then two triangles\n",
            "circle after triangle\n",
            "a circle then a circle\n",
            "two circles\n",
            "circle after triangle and one circle\n",
            "circle after triangle and a triangle\n",
            "circle after circle then triangle after circle\n",
            "triangle after triangle and a circle\n",
            "two triangles\n",
            "two circles\n",
            "triangle after circle and two triangles\n",
            "circle after circle and a circle\n",
            "a triangle then a circle then circle after triangle\n",
            "one triangle\n",
            "one circle and a triangle\n",
            "a circle\n",
            "circle after triangle then a triangle\n",
            "two circles\n",
            "one circle\n",
            "two circles\n",
            "a triangle\n",
            "a triangle\n",
            "triangle after triangle then a triangle and one circle\n",
            "a circle and two triangles\n",
            "one circle\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "SHAPES = [\"triangle\", \"circle\"]\n",
        "PLURALS = [shape + \"s\" for shape in SHAPES]\n",
        "ARTICLES = [\"a\", \"one\"]\n",
        "TWO_ARTICLES = [\"two\"]\n",
        "CONNECTORS = [\"and\", \"then\"]\n",
        "REVERSE_CONNECTORS = [\"after\"]\n",
        "CLASS_TOKEN = \"CLS\"\n",
        "MASK_TOKEN = \"MASK\"\n",
        "SEP_TOKEN = \"SEP\"\n",
        "PAD_TOKEN = \"PAD\"\n",
        "EOS_TOKEN = \"EOS\"\n",
        "SPECIAL_TOKENS = [CLASS_TOKEN, MASK_TOKEN, SEP_TOKEN, PAD_TOKEN, EOS_TOKEN]\n",
        "VOCABULARY = SHAPES + PLURALS + ARTICLES + CONNECTORS + REVERSE_CONNECTORS + TWO_ARTICLES + SPECIAL_TOKENS\n",
        "MAX_LEN_SENTENCE = 16 # Maximum possible length of a sequence\n",
        "\n",
        "def generate_descr_from_list(r):\n",
        "  if len(r) > 4:\n",
        "    raise Exception(\"Too many items\")\n",
        "  elif len(r) == 0:\n",
        "    return \"\"\n",
        "  elif len(r) == 1:\n",
        "    article = random.choice(ARTICLES)\n",
        "    return \"{} {}\".format(article, r[0])\n",
        "  else:\n",
        "    reversed_descr = random.random() > 0.5\n",
        "    if reversed_descr:\n",
        "      descr = \"{} {} {}\".format(r[1], random.choice(REVERSE_CONNECTORS), r[0])\n",
        "      if len(r) > 2:\n",
        "        return descr + \" \" + random.choice(CONNECTORS) + \" \" + generate_descr_from_list(r[2:])\n",
        "      return descr\n",
        "    if r[0] == r[1]:\n",
        "      plural_desc = random.random() > 0.5\n",
        "      if plural_desc:\n",
        "          return \"{} {}s\".format(random.choice(TWO_ARTICLES), r[0])\n",
        "    return generate_descr_from_list([r[0]]) + \" \" + random.choice(CONNECTORS) + \" \" + generate_descr_from_list(r[1:])\n",
        "\n",
        "\n",
        "def generate_random_shapes():\n",
        "  num_shapes = random.randint(1, 4)\n",
        "  result = []\n",
        "  for _ in range(num_shapes):\n",
        "    result.append(random.choice(SHAPES))\n",
        "  return result\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "  print(generate_descr_from_list(generate_random_shapes()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import random\n",
        "\n",
        "def generate_image(word_list, filename):\n",
        "    # Create a blank 32x32 image\n",
        "    image_size = 32\n",
        "    patch_size = 16\n",
        "    image = Image.new(\"1\", (image_size, image_size), 1)  # '1' for 1-bit pixels, black and white\n",
        "\n",
        "    for i, word in enumerate(word_list):\n",
        "        if word not in [\"triangle\", \"circle\"]:\n",
        "            continue\n",
        "\n",
        "        # Determine the top-left corner of the patch\n",
        "        x_offset = (i % 2) * patch_size\n",
        "        y_offset = (i // 2) * patch_size\n",
        "\n",
        "        # Draw the shape in the corresponding patch\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        if word == \"triangle\":\n",
        "            points = [(random.randint(x_offset, x_offset + patch_size), random.randint(y_offset, y_offset + patch_size)) for _ in range(3)]\n",
        "            draw.polygon(points, fill=0)\n",
        "        elif word == \"circle\":\n",
        "            radius = random.randint(2, patch_size // 2)\n",
        "            center_x = random.randint(x_offset + radius, x_offset + patch_size - radius)\n",
        "            center_y = random.randint(y_offset + radius, y_offset + patch_size - radius)\n",
        "            draw.ellipse([center_x - radius, center_y - radius, center_x + radius, center_y + radius], fill=0)\n",
        "\n",
        "    # Save the image to the specified filename\n",
        "    image.save(filename)\n",
        "\n",
        "def plot_image(filename):\n",
        "    # Open the image\n",
        "    image = Image.open(filename)\n",
        "\n",
        "    # Convert the image to a NumPy array\n",
        "    image_array = np.array(image)\n",
        "\n",
        "    # Plot the image\n",
        "    plt.imshow(image_array)\n",
        "    plt.axis('off')  # Turn off axis labels\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "generate_image([\"circle\", \"triangle\", \"circle\"], \"output_image.png\")\n"
      ],
      "metadata": {
        "id": "wTQhOfTfnv11"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBER_CODES = [str(i) for i in range(5)]\n",
        "SHAPE_CODES = [shape[0] for shape in SHAPES]\n",
        "CODES = NUMBER_CODES + SHAPE_CODES\n",
        "CODE_VOCABULARY = SPECIAL_TOKENS + CODES\n",
        "\n",
        "def generate_code_str(shape_list):\n",
        "  codes = []\n",
        "  i = 0\n",
        "  while i < len(shape_list):\n",
        "    j = i + 1\n",
        "    while j < len(shape_list) and shape_list[i] == shape_list[j]:\n",
        "      j += 1\n",
        "    codes.append(f\"{NUMBER_CODES[j-i]} {shape_list[i][0]}\")\n",
        "    i = j\n",
        "  return \" \".join(codes)"
      ],
      "metadata": {
        "id": "fKuFwFcvmroH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(n_samples):\n",
        "  examples = []\n",
        "  for i in range(n_samples):\n",
        "    shape_list = generate_random_shapes()\n",
        "    code_str = generate_code_str(shape_list)\n",
        "    text = generate_descr_from_list(shape_list)\n",
        "    filename = f\"f{i}.png\"\n",
        "    generate_image(shape_list, filename)\n",
        "    examples.append({\"shape_list\": shape_list, \"code_str\": code_str, \"text\": text, \"filename\": filename})\n",
        "  return examples\n"
      ],
      "metadata": {
        "id": "VvfchFGDpLHC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "# Step 1: Prepare the Dataset\n",
        "class CountingFiguresDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "TRF1Xd44rZsx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88p-uxGSSEbs",
        "outputId": "552d195c-5e77-421f-856d-939ef7b45c3d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m789.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from collections import OrderedDict\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "class MyTokenizer:\n",
        "    def __init__(self, vocabulary):\n",
        "        self.vocabulary = vocabulary\n",
        "\n",
        "    def encode(self, text, add_special_tokens=True, max_length=MAX_LEN_SENTENCE, return_token_type_ids=False, padding='max_length', return_attention_mask=True, return_tensors='pt'):\n",
        "        tokens = text.split()\n",
        "        tokens.append(EOS_TOKEN)\n",
        "        input_ids = [self.vocabulary.index(token) for token in tokens]\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        if add_special_tokens:\n",
        "            input_ids = [self.vocabulary.index(CLASS_TOKEN)] + input_ids\n",
        "            attention_mask += [1]\n",
        "\n",
        "        sen_len = len(input_ids)\n",
        "        if len(input_ids) > max_length:\n",
        "            input_ids = input_ids[:max_length]\n",
        "            attention_mask = attention_mask[:max_length]\n",
        "            sen_len = max_length\n",
        "        else:\n",
        "            pad_length = max_length - len(input_ids)\n",
        "            if pad_length >= 0:\n",
        "                input_ids += [self.vocabulary.index(PAD_TOKEN)] * pad_length\n",
        "                attention_mask += [0] * pad_length\n",
        "\n",
        "        return sen_len, input_ids, attention_mask\n",
        "\n",
        "    def encode_plus(self, text, add_special_tokens=True, max_length=MAX_LEN_SENTENCE, return_token_type_ids=False, padding='max_length', return_attention_mask=True, return_tensors='pt'):\n",
        "        _, input_ids, attention_mask = self.encode(text, add_special_tokens, max_length, return_token_type_ids, padding, return_attention_mask, return_tensors)\n",
        "        if return_attention_mask:\n",
        "          return {\n",
        "              'input_ids': torch.tensor(input_ids),\n",
        "              'attention_mask': torch.tensor(attention_mask)\n",
        "          }\n",
        "        else:\n",
        "          return {\n",
        "              'input_ids': torch.tensor(input_ids)\n",
        "          }\n"
      ],
      "metadata": {
        "id": "S1ey-4OGDwWF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_key_dim, hidden_val_dim, enc_emb_dim=None, num_heads=1):\n",
        "        \"\"\"\n",
        "          Implements an attention mechanism\n",
        "\n",
        "          Args:\n",
        "          input_dim: Dimensionality of input embedding.\n",
        "          hidden_key_dim: Dimensionality of key and query vectors.\n",
        "          hidden_val_dim: Dimensionality of value vectors.\n",
        "          enc_emb_dim: Dimensionality of encoder embeddings. If None, self-attention is used.\n",
        "          mask: Whether to apply masking. If True, the attention scores for masked positions are set to -inf.\n",
        "          num_heads: Number of attention heads.\n",
        "        \"\"\"\n",
        "        super(MyAttention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.enc_emb_dim = enc_emb_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.to_q = nn.Linear(self.input_dim, self.hidden_key_dim * self.num_heads, bias=False)\n",
        "\n",
        "        for i in range(self.num_heads):\n",
        "          if enc_emb_dim is None:\n",
        "              self.to_k = nn.Linear(self.input_dim, self.hidden_key_dim * self.num_heads, bias=False)\n",
        "              self.to_v = nn.Linear(self.input_dim, self.hidden_val_dim * self.num_heads, bias=False)\n",
        "          else:\n",
        "              self.to_k = nn.Linear(self.enc_emb_dim, self.hidden_key_dim * self.num_heads, bias=False)\n",
        "              self.to_v = nn.Linear(self.enc_emb_dim, self.hidden_val_dim * self.num_heads, bias=False)\n",
        "\n",
        "        self.to_out = nn.Linear(self.hidden_val_dim * self.num_heads, self.input_dim)\n",
        "\n",
        "    def forward(self, embeddings, encoder_embeddings=None, attention_mask=None):\n",
        "\n",
        "        if encoder_embeddings is not None and attention_mask is not None:\n",
        "            raise Exception(\"In cross attention there is no masking.\")\n",
        "\n",
        "        if encoder_embeddings is None:\n",
        "            Q = self.to_q(embeddings)\n",
        "            K = self.to_k(embeddings)\n",
        "            V = self.to_v(embeddings)\n",
        "        else:\n",
        "            Q = self.to_q(embeddings)\n",
        "            K = self.to_k(encoder_embeddings)\n",
        "            V = self.to_v(encoder_embeddings)\n",
        "\n",
        "        Q = rearrange(Q, 'B T (H D) -> B H T D', H=self.num_heads, D=self.hidden_key_dim)\n",
        "        K = rearrange(K, 'B T (H D) -> B H T D', H=self.num_heads, D=self.hidden_key_dim)\n",
        "        V = rearrange(V, 'B T (H D) -> B H T D', H=self.num_heads, D=self.hidden_val_dim)\n",
        "\n",
        "        scores = torch.einsum(\"BHTD,BHSD->BHTS\", Q, K)\n",
        "\n",
        "        if attention_mask is not None and encoder_embeddings is None:\n",
        "            # Originally, attention_mask has shape (batch_size, sequence_len)\n",
        "            # To ensure propagation to the scores matrix, which has shape (batch_size, num_heads, sequence_len, sequence_len),\n",
        "            # We need to make attention_mask's shape (batch_size, 1, 1, sequence_len)\n",
        "            # We do this with the unsqueeze method, which adds a new dimension.\n",
        "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
        "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
        "\n",
        "        attnmats = F.softmax(scores / math.sqrt(self.hidden_key_dim), dim=-1)\n",
        "\n",
        "        ctx_vecs = torch.einsum(\"BHTS,BHSD->BHTD\", attnmats, V)\n",
        "        ctx_vecs = rearrange(ctx_vecs, 'B H T D -> B T (H D)', H=self.num_heads, D=self.hidden_val_dim)\n",
        "        return self.to_out(ctx_vecs)"
      ],
      "metadata": {
        "id": "aFYriqViEvVt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=MAX_LEN_SENTENCE):\n",
        "        super(MyPositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a matrix of shape (max_len, d_model) with all zeros\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Create a column vector of shape (max_len, 1) with values [0, 1, ..., max_len-1]\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Create a row vector of shape (1, d_model // 2) with values [0, 1, ..., d_model//2-1]\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even indices and cosine to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add a batch dimension (1, max_len, d_model) and register as buffer\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input tensor (B, T, D)\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "HJBjsyuMMeLO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_key_dim, hidden_val_dim, output_dim, num_heads=1):\n",
        "        super(MyTransformerEncoderLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.attention = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=None, num_heads=self.num_heads)\n",
        "        self.norm1 = nn.LayerNorm(self.input_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, self.output_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(self.output_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        x = self.norm1(self.attention(x, attention_mask=attention_mask) + x)\n",
        "        x = self.norm2(self.feed_forward(x) + x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jt6_PYhSNLBh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_tokens, input_dim, hidden_key_dim, hidden_val_dim, output_dim, max_length, num_layers=1, num_heads=1):\n",
        "        super(MyTransformerEncoder, self).__init__()\n",
        "        self.num_tokens = num_tokens\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.max_length = max_length\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.embedding = nn.Embedding(num_tokens, self.input_dim)\n",
        "        self.positional_encoding = MyPositionalEncoding(self.input_dim, max_length)\n",
        "        self.layers = nn.ModuleList([MyTransformerEncoderLayer(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, self.input_dim, num_heads) for _ in range(num_layers)])\n",
        "        self.linear = nn.Linear(self.input_dim, self.output_dim)\n",
        "        self.norm = nn.LayerNorm(self.output_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask=attention_mask)\n",
        "        x = self.linear(x)\n",
        "        x = self.norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "N90NAhGjMgZu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerClassifier(nn.Module):\n",
        "    def __init__(self, transf_enc, num_classes):\n",
        "        super(MyTransformerClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.transf_enc = transf_enc\n",
        "\n",
        "        self.linear = nn.Linear(self.transf_enc.output_dim, self.num_classes)\n",
        "\n",
        "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
        "        x = self.transf_enc(input_ids, attention_mask)[:, 0, :] # Just the embedding of the first token, which is the CLS token.\n",
        "        logits = self.linear(x)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss = criterion(logits, labels)\n",
        "        return (loss, logits) if loss is not None else logits"
      ],
      "metadata": {
        "id": "thRPUXH4PtXH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "\n",
        "# Step 2: Tokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer = MyTokenizer(VOCABULARY)\n",
        "tokenizer.encode_plus(\n",
        "            \"one circle after one circle and two triangles\",\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN_SENTENCE,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUw3D9j26rgn",
        "outputId": "b2f39fd1-7ed0-40f5-eca8-d55f0fd95fba"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([10,  5,  1,  8,  5,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 100\n",
        "data = generate_data(n_samples)\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dlKZJQsCwew",
        "outputId": "52a6cca6-1fcd-431d-e392-97ade65ee042"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['shape_list', 'code_str', 'text', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset\n",
        "descriptions = [d[\"text\"] for d in data]\n",
        "lengths = [len(d[\"shape_list\"]) for d in data]\n",
        "dataset = CountingFiguresDataset(descriptions, lengths, tokenizer, max_length=MAX_LEN_SENTENCE)"
      ],
      "metadata": {
        "id": "kgwzsdVxJIdL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(VOCABULARY)\n",
        "input_dim = 16\n",
        "hidden_key_dim = 8\n",
        "hidden_val_dim = 8\n",
        "num_heads = 2\n",
        "output_dim = 16\n",
        "num_layers = 3\n",
        "num_labels = 5\n",
        "\n",
        "# Step 3: Model\n",
        "transf_enc = MyTransformerEncoder(num_tokens, input_dim=input_dim, hidden_key_dim=hidden_key_dim, hidden_val_dim=hidden_val_dim, output_dim=output_dim, max_length=MAX_LEN_SENTENCE, num_layers=num_layers, num_heads=num_heads)\n",
        "model = MyTransformerClassifier(transf_enc, num_labels)\n",
        "\n",
        "# Step 4: Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=200,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.001,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Split dataset into train and eval\n",
        "train_size = int(0.8 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
        "\n",
        "# Custom Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "DQbq5O20F1Nx",
        "outputId": "0e96b8bf-595d-44b4-d27b-a87f5014c4ba"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='202' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 202/8000 00:03 < 02:09, 60.12 it/s, Epoch 5.03/200]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.869100</td>\n",
              "      <td>1.980361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.881500</td>\n",
              "      <td>1.899939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.619100</td>\n",
              "      <td>1.828854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.631700</td>\n",
              "      <td>1.770872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.323400</td>\n",
              "      <td>1.717627</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e374a741942e>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1932\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1933\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1934\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3306\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3307\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3309\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3336\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-495b3f36521e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, labels, attention_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransf_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Just the embedding of the first token, which is the CLS token.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-6edbc7f31a14>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-50c672874dd3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-0edc1d2b90f2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, encoder_embeddings, attention_mask)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mctx_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BHTS,BHSD->BHTD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattnmats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mctx_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B H T D -> B T (H D)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_val_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Step 5: Evaluation\n",
        "def evaluate_model(texts, labels):\n",
        "    eval_dataset = CountingFiguresDataset(texts, labels, tokenizer, max_length=MAX_LEN_SENTENCE)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=2)\n",
        "    total_correct = 0\n",
        "    total_samples = len(labels)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=-1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "# Generate descriptions and images for each shape list\n",
        "\n",
        "test_shape_lists = [generate_random_shapes() for _ in range(100)]\n",
        "\n",
        "eval_descriptions = []\n",
        "eval_lengths = []\n",
        "for i, shape_list in enumerate(test_shape_lists):\n",
        "  eval_descriptions.append(generate_descr_from_list(shape_list))\n",
        "  eval_lengths.append(len(shape_list))\n",
        "\n",
        "evaluate_model(eval_descriptions, eval_lengths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4j1RRLb7hzX",
        "outputId": "bb6a454e-304b-4bb8-d046-f949e484e09f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 26.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_key_dim, hidden_val_dim, output_dim, enc_emb_dim, num_heads):\n",
        "        super(MyTransformerDecoderLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.enc_emb_dim = enc_emb_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.masked_att = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=None, num_heads=self.num_heads)\n",
        "        self.norm1 = nn.LayerNorm(self.input_dim)\n",
        "        self.cross_att = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=self.enc_emb_dim, num_heads=self.num_heads)\n",
        "        self.norm2 = nn.LayerNorm(self.input_dim)\n",
        "        self.self_att = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=None, num_heads=self.num_heads)\n",
        "        self.norm3 = nn.LayerNorm(self.input_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, self.output_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, enc_emb, attention_mask):\n",
        "        x = self.norm1(x + self.masked_att(x, attention_mask=attention_mask))\n",
        "        x = self.norm2(x + self.cross_att(x, encoder_embeddings=enc_emb))\n",
        "        x = self.norm3(x + self.self_att(x, attention_mask=attention_mask))\n",
        "        x = self.feed_forward(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jeRheKQGyfmO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_tokens, input_dim, hidden_key_dim, hidden_val_dim, output_dim, enc_emb_dim, max_length, num_layers, num_heads):\n",
        "        super(MyTransformerDecoder, self).__init__()\n",
        "        self.num_tokens = num_tokens\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.enc_emb_dim = enc_emb_dim\n",
        "        self.max_length = max_length\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.embedding = nn.Embedding(num_tokens, self.input_dim)\n",
        "        self.positional_encoding = MyPositionalEncoding(self.input_dim, max_length)\n",
        "        self.layers = nn.ModuleList([MyTransformerDecoderLayer(input_dim=self.input_dim,\n",
        "                                                               hidden_key_dim=self.hidden_key_dim,\n",
        "                                                               hidden_val_dim=self.hidden_val_dim,\n",
        "                                                               enc_emb_dim=self.enc_emb_dim,\n",
        "                                                               output_dim=self.output_dim,\n",
        "                                                               num_heads=num_heads) for _ in range(num_layers)])\n",
        "        self.linear = nn.Linear(self.input_dim, self.output_dim)\n",
        "        self.norm = nn.LayerNorm(self.output_dim)\n",
        "\n",
        "    def forward(self, x, enc_emb, attention_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_emb, attention_mask=attention_mask)\n",
        "        x = self.linear(x)\n",
        "        x = self.norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ztvWkEGivzxx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerTranslator(nn.Module):\n",
        "    def __init__(self, transf_enc, transf_dec, num_tokens_target_vocab):\n",
        "        super(MyTransformerTranslator, self).__init__()\n",
        "        self.transf_enc = transf_enc\n",
        "        self.transf_dec = transf_dec\n",
        "        self.linear = nn.Linear(self.transf_dec.output_dim, num_tokens_target_vocab)\n",
        "\n",
        "    def forward(self, source_tokens, target_tokens, attention_mask, labels=None):\n",
        "        source_embeddings = self.transf_enc(source_tokens)\n",
        "        decoded_embeddings = self.transf_dec(target_tokens, source_embeddings, attention_mask)[:, 0, :]\n",
        "        logits = self.linear(decoded_embeddings)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss = criterion(logits, labels)\n",
        "        return (loss, logits) if loss is not None else logits"
      ],
      "metadata": {
        "id": "LLudUsAtwhAs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MyRandomMaskTokenizer(MyTokenizer):\n",
        "  def __init__(self, vocabulary):\n",
        "    super().__init__(vocabulary)\n",
        "\n",
        "  def encode_plus(self, text, add_special_tokens=True, max_length=MAX_LEN_SENTENCE, return_token_type_ids=False, padding='max_length', return_attention_mask=True, return_tensors='pt'):\n",
        "    sen_len, input_ids, attention_mask = super().encode(text, add_special_tokens, max_length, return_token_type_ids, padding, return_attention_mask, return_tensors)\n",
        "    if return_attention_mask:\n",
        "      new_len = random.randint(1, sen_len-1)\n",
        "      new_attention_mask = torch.cat((torch.ones(new_len), torch.zeros(len(input_ids) - new_len)))\n",
        "      return {\n",
        "              'input_ids': torch.tensor(input_ids),\n",
        "              'attention_mask': new_attention_mask,\n",
        "              'sen_len': new_len\n",
        "      }\n",
        "    else:\n",
        "      return {\n",
        "              'input_ids': torch.tensor(input_ids),\n",
        "              'sen_len': new_len\n",
        "      }\n",
        "\n",
        "tokenizer = MyRandomMaskTokenizer(CODE_VOCABULARY)\n",
        "tokenizer.encode_plus(\n",
        "            \"c 2 t 2\",\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN_SENTENCE,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvU4FiMZzYUU",
        "outputId": "edaadda0-d3fd-48a6-ac29-fbc57ced22c2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 0, 11,  7, 10,  7,  4,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3]),\n",
              " 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'sen_len': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_texts, tgt_texts, src_tokenizer, tgt_tokenizer, src_max_len, tgt_max_len):\n",
        "        self.src_texts = src_texts\n",
        "        self.tgt_texts = tgt_texts\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.src_max_len = src_max_len\n",
        "        self.tgt_max_len = tgt_max_len\n",
        "\n",
        "        self.tgt_encodings = [self.tgt_tokenizer.encode_plus(\n",
        "              txt,\n",
        "              add_special_tokens=True,\n",
        "              max_length=self.tgt_max_len,\n",
        "              return_token_type_ids=False,\n",
        "              padding='max_length',\n",
        "              return_attention_mask=True,\n",
        "              return_tensors='pt'\n",
        "            ) for txt in self.tgt_texts]\n",
        "\n",
        "        self.labels=[]\n",
        "        for tgt_encoding in self.tgt_encodings:\n",
        "          input_ids = tgt_encoding['input_ids']\n",
        "          label = input_ids[tgt_encoding['sen_len']]\n",
        "          self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_encoding = self.src_tokenizer.encode_plus(\n",
        "            self.src_texts[idx],\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.src_max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'source_tokens': src_encoding['input_ids'].flatten(),\n",
        "            'target_tokens': self.tgt_encodings[idx]['input_ids'].flatten(),\n",
        "            'attention_mask': self.tgt_encodings[idx]['attention_mask'].flatten(),\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "zgAr-duaEWP1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CODE_LEN=8\n",
        "\n",
        "n_examples = 100\n",
        "data = generate_data(n_examples)\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPNYvGpfUVI9",
        "outputId": "98eb77f1-86e6-415f-c38e-de34553542f9"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['shape_list', 'code_str', 'text', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "descriptions = [d[\"text\"] for d in data]\n",
        "code_lists = [d[\"code_str\"] for d in data]"
      ],
      "metadata": {
        "id": "ujIdHu501lA2"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transl_dataset = TranslationDataset(descriptions, code_lists, MyTokenizer(VOCABULARY), MyRandomMaskTokenizer(CODE_VOCABULARY), MAX_LEN_SENTENCE, tgt_max_len=MAX_CODE_LEN)\n",
        "for example in transl_dataset:\n",
        "  print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4OF45l51jYX",
        "outputId": "951baa3a-6cec-4df8-f5b2-924d1d3ac49c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  4,  0,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  1,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  7, 11,  4]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  5,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  5,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  5,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  0,  8,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  4,  0,  7,  5,  1,  6,  5,  0, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  4,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  5,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  4,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  5,  1,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  4,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  4,  0,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  4,  0,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_src_tokens=len(VOCABULARY)\n",
        "num_tgt_tokens=len(CODE_VOCABULARY)\n",
        "input_dim = 16\n",
        "hidden_key_dim = 8\n",
        "hidden_val_dim = 8\n",
        "num_heads = 3\n",
        "output_dim = 16\n",
        "num_layers = 3\n",
        "\n",
        "transf_enc = MyTransformerEncoder(num_src_tokens, input_dim=input_dim, hidden_key_dim=hidden_key_dim, hidden_val_dim=hidden_val_dim, output_dim=output_dim, max_length=MAX_LEN_SENTENCE, num_layers=num_layers, num_heads=num_heads)\n",
        "transf_dec = MyTransformerDecoder(num_tgt_tokens, input_dim=input_dim, hidden_key_dim=hidden_key_dim, hidden_val_dim=hidden_val_dim, output_dim=output_dim, enc_emb_dim=output_dim, max_length=MAX_CODE_LEN, num_layers=num_layers, num_heads=num_heads)\n",
        "model = MyTransformerTranslator(transf_enc, transf_dec, len(CODE_VOCABULARY))\n",
        "\n",
        "# Step 4: Training\n",
        "num_epochs = 100\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.001,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Split dataset into train and eval\n",
        "train_size = int(0.8 * len(transl_dataset))\n",
        "eval_size = len(transl_dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(transl_dataset, [train_size, eval_size])\n",
        "\n",
        "# Custom Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MGMACM7bVOnt",
        "outputId": "91e090cb-fb7e-40e5-8a21-cee9efe56aa4"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4000/4000 02:44, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.375800</td>\n",
              "      <td>2.455026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.497000</td>\n",
              "      <td>2.277584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.942300</td>\n",
              "      <td>2.096743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.991100</td>\n",
              "      <td>1.964074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.853000</td>\n",
              "      <td>1.939399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.943400</td>\n",
              "      <td>1.919752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.839300</td>\n",
              "      <td>1.918899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.041100</td>\n",
              "      <td>1.898992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.807600</td>\n",
              "      <td>1.886360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.784500</td>\n",
              "      <td>1.874993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.647000</td>\n",
              "      <td>1.868916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.681800</td>\n",
              "      <td>1.854870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.760500</td>\n",
              "      <td>1.843850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.658900</td>\n",
              "      <td>1.838412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.729300</td>\n",
              "      <td>1.828562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.628000</td>\n",
              "      <td>1.827674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.852000</td>\n",
              "      <td>1.821773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.483500</td>\n",
              "      <td>1.813978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.522300</td>\n",
              "      <td>1.811797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.891200</td>\n",
              "      <td>1.806481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.634600</td>\n",
              "      <td>1.800585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.833100</td>\n",
              "      <td>1.796224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.471500</td>\n",
              "      <td>1.787244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.399100</td>\n",
              "      <td>1.783377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.628100</td>\n",
              "      <td>1.774503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.269200</td>\n",
              "      <td>1.767147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.587800</td>\n",
              "      <td>1.758749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.654500</td>\n",
              "      <td>1.760185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.496600</td>\n",
              "      <td>1.754304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.390100</td>\n",
              "      <td>1.754724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.752700</td>\n",
              "      <td>1.746525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.710200</td>\n",
              "      <td>1.737978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.567300</td>\n",
              "      <td>1.734976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.479300</td>\n",
              "      <td>1.746697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.403500</td>\n",
              "      <td>1.720725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.478500</td>\n",
              "      <td>1.716019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.495700</td>\n",
              "      <td>1.708166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.407800</td>\n",
              "      <td>1.705069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.574600</td>\n",
              "      <td>1.698366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.513700</td>\n",
              "      <td>1.691504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.409700</td>\n",
              "      <td>1.696939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.551600</td>\n",
              "      <td>1.678854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.397000</td>\n",
              "      <td>1.680760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.402400</td>\n",
              "      <td>1.663812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>1.667554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.296800</td>\n",
              "      <td>1.657246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.370000</td>\n",
              "      <td>1.649565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.580400</td>\n",
              "      <td>1.651694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.336500</td>\n",
              "      <td>1.649817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.220200</td>\n",
              "      <td>1.644177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.371100</td>\n",
              "      <td>1.647677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.332600</td>\n",
              "      <td>1.631804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.352400</td>\n",
              "      <td>1.630466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.380400</td>\n",
              "      <td>1.625685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.592000</td>\n",
              "      <td>1.621097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.299400</td>\n",
              "      <td>1.613167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.206600</td>\n",
              "      <td>1.615211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.361100</td>\n",
              "      <td>1.614725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.456200</td>\n",
              "      <td>1.607229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.534400</td>\n",
              "      <td>1.606190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.297100</td>\n",
              "      <td>1.604339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.262100</td>\n",
              "      <td>1.599337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.361500</td>\n",
              "      <td>1.600240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.164700</td>\n",
              "      <td>1.598910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.295100</td>\n",
              "      <td>1.597067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.391900</td>\n",
              "      <td>1.593807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.402300</td>\n",
              "      <td>1.592553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.329200</td>\n",
              "      <td>1.587151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.240700</td>\n",
              "      <td>1.585338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.244000</td>\n",
              "      <td>1.587408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.035100</td>\n",
              "      <td>1.580613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.210200</td>\n",
              "      <td>1.580962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.392300</td>\n",
              "      <td>1.580561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.275500</td>\n",
              "      <td>1.577394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.428700</td>\n",
              "      <td>1.573514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.283900</td>\n",
              "      <td>1.574646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.175200</td>\n",
              "      <td>1.571457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.216700</td>\n",
              "      <td>1.572853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.185400</td>\n",
              "      <td>1.570357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.284200</td>\n",
              "      <td>1.565717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.448200</td>\n",
              "      <td>1.565729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.094600</td>\n",
              "      <td>1.564685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.225700</td>\n",
              "      <td>1.564439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.309100</td>\n",
              "      <td>1.567105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.304800</td>\n",
              "      <td>1.564575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.152100</td>\n",
              "      <td>1.563391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.136100</td>\n",
              "      <td>1.561521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.299500</td>\n",
              "      <td>1.563012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.122800</td>\n",
              "      <td>1.562229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.520300</td>\n",
              "      <td>1.561073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.340700</td>\n",
              "      <td>1.559471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.216600</td>\n",
              "      <td>1.559099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.331900</td>\n",
              "      <td>1.558236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.201900</td>\n",
              "      <td>1.558713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.399800</td>\n",
              "      <td>1.559320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.200600</td>\n",
              "      <td>1.558676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.351800</td>\n",
              "      <td>1.558140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.093600</td>\n",
              "      <td>1.557677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.436400</td>\n",
              "      <td>1.558199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.077000</td>\n",
              "      <td>1.557961</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4000, training_loss=1.4473476045131684, metrics={'train_runtime': 164.4301, 'train_samples_per_second': 48.653, 'train_steps_per_second': 24.326, 'total_flos': 0.0, 'train_loss': 1.4473476045131684, 'epoch': 100.0})"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_examples = 200\n",
        "data = generate_data(n_examples)\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZjq73ar0qWf",
        "outputId": "a67e4254-16f7-4e7a-8b71-7bc44cc5f0ad"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['shape_list', 'code_str', 'text', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [d[\"text\"] for d in data]\n",
        "code_lists = [d[\"code_str\"] for d in data]"
      ],
      "metadata": {
        "id": "ia4ctxrX04LE"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = TranslationDataset(texts, code_lists, MyTokenizer(VOCABULARY), MyRandomMaskTokenizer(CODE_VOCABULARY), MAX_LEN_SENTENCE, tgt_max_len=MAX_CODE_LEN)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=2)\n",
        "total_correct = 0\n",
        "total_samples = len(texts)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in eval_loader:\n",
        "        src_tk = batch['source_tokens']\n",
        "        tgt_tk = batch['target_tokens']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        outputs = model(source_tokens=src_tk, target_tokens=tgt_tk, attention_mask=attention_mask)\n",
        "        print(f\"labels: {labels}\")\n",
        "        print(f\"preds: {preds}\")\n",
        "        _, preds = torch.max(outputs, dim=-1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHnXexw-WNG9",
        "outputId": "e5988252-da95-45d1-90b5-d3a6d28f3e3b"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels: tensor([ 6, 10])\n",
            "preds: tensor([6, 4])\n",
            "labels: tensor([11,  6])\n",
            "preds: tensor([ 6, 10])\n",
            "labels: tensor([10,  4])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([10, 11])\n",
            "preds: tensor([11,  4])\n",
            "labels: tensor([8, 4])\n",
            "preds: tensor([10, 11])\n",
            "labels: tensor([10,  4])\n",
            "preds: tensor([6, 4])\n",
            "labels: tensor([11,  6])\n",
            "preds: tensor([11,  4])\n",
            "labels: tensor([6, 7])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([8, 6])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([4, 6])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([11,  4])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([4, 7])\n",
            "preds: tensor([11,  4])\n",
            "labels: tensor([4, 6])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([6, 8])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([11,  8])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([ 4, 10])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([4, 4])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([11,  8])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([7, 6])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([11, 10])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([ 4, 11])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([ 4, 11])\n",
            "preds: tensor([ 4, 11])\n",
            "labels: tensor([ 6, 10])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([10,  6])\n",
            "preds: tensor([ 6, 11])\n",
            "labels: tensor([6, 4])\n",
            "preds: tensor([10,  4])\n",
            "labels: tensor([6, 6])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([4, 4])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([6, 6])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([10, 11])\n",
            "preds: tensor([6, 4])\n",
            "labels: tensor([4, 6])\n",
            "preds: tensor([ 6, 11])\n",
            "labels: tensor([11,  4])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([4, 6])\n",
            "preds: tensor([11,  4])\n",
            "labels: tensor([ 7, 10])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([11,  7])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([9, 6])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([10,  4])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([ 4, 11])\n",
            "preds: tensor([11,  4])\n",
            "labels: tensor([11,  4])\n",
            "preds: tensor([ 4, 11])\n",
            "labels: tensor([6, 4])\n",
            "preds: tensor([11,  4])\n",
            "labels: tensor([6, 7])\n",
            "preds: tensor([ 6, 11])\n",
            "labels: tensor([11,  4])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([4, 6])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([4, 4])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([6, 6])\n",
            "preds: tensor([6, 4])\n",
            "labels: tensor([11, 11])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([6, 4])\n",
            "preds: tensor([11,  4])\n",
            "labels: tensor([4, 7])\n",
            "preds: tensor([6, 4])\n",
            "labels: tensor([11,  4])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([10,  7])\n",
            "preds: tensor([11,  4])\n",
            "labels: tensor([6, 4])\n",
            "preds: tensor([10,  4])\n",
            "labels: tensor([10, 11])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([11, 11])\n",
            "preds: tensor([11, 11])\n",
            "labels: tensor([6, 4])\n",
            "preds: tensor([11, 11])\n",
            "labels: tensor([4, 4])\n",
            "preds: tensor([11, 11])\n",
            "labels: tensor([ 7, 11])\n",
            "preds: tensor([11,  4])\n",
            "labels: tensor([11, 11])\n",
            "preds: tensor([ 6, 11])\n",
            "labels: tensor([11, 11])\n",
            "preds: tensor([11, 11])\n",
            "labels: tensor([6, 6])\n",
            "preds: tensor([11, 11])\n",
            "labels: tensor([4, 4])\n",
            "preds: tensor([6, 4])\n",
            "labels: tensor([11, 10])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([10,  6])\n",
            "preds: tensor([10, 11])\n",
            "labels: tensor([6, 4])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([4, 7])\n",
            "preds: tensor([6, 4])\n",
            "labels: tensor([4, 7])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([10, 11])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([ 7, 10])\n",
            "preds: tensor([ 4, 11])\n",
            "labels: tensor([7, 8])\n",
            "preds: tensor([6, 4])\n",
            "labels: tensor([10, 10])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([4, 4])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([6, 6])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([ 4, 11])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([4, 7])\n",
            "preds: tensor([ 4, 11])\n",
            "labels: tensor([4, 6])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([ 4, 10])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([ 9, 10])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([4, 4])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([11, 10])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([10,  4])\n",
            "preds: tensor([11, 11])\n",
            "labels: tensor([ 4, 11])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([ 6, 11])\n",
            "preds: tensor([ 4, 11])\n",
            "labels: tensor([4, 7])\n",
            "preds: tensor([11,  4])\n",
            "labels: tensor([ 4, 10])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([11,  4])\n",
            "preds: tensor([11, 11])\n",
            "labels: tensor([7, 4])\n",
            "preds: tensor([11, 11])\n",
            "labels: tensor([10, 10])\n",
            "preds: tensor([6, 6])\n",
            "labels: tensor([10,  7])\n",
            "preds: tensor([11, 11])\n",
            "labels: tensor([6, 4])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([11,  4])\n",
            "preds: tensor([6, 4])\n",
            "labels: tensor([7, 4])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([4, 4])\n",
            "preds: tensor([6, 4])\n",
            "labels: tensor([4, 7])\n",
            "preds: tensor([4, 4])\n",
            "labels: tensor([ 8, 10])\n",
            "preds: tensor([11,  6])\n",
            "labels: tensor([10,  9])\n",
            "preds: tensor([6, 4])\n",
            "labels: tensor([ 6, 11])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([10,  4])\n",
            "preds: tensor([ 6, 11])\n",
            "labels: tensor([11,  6])\n",
            "preds: tensor([11,  4])\n",
            "labels: tensor([ 6, 10])\n",
            "preds: tensor([4, 6])\n",
            "labels: tensor([10, 10])\n",
            "preds: tensor([ 6, 10])\n",
            "labels: tensor([7, 4])\n",
            "preds: tensor([10, 11])\n",
            "labels: tensor([6, 4])\n",
            "preds: tensor([6, 4])\n",
            "Accuracy: 58.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text, src_tokenizer, tgt_tokenizer, src_max_length, tgt_max_length):\n",
        "    src_tokens = src_tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=src_max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "    translation = \"\"\n",
        "    tgt_tokens = tgt_tokenizer.encode_plus(\n",
        "            translation,\n",
        "            add_special_tokens=True,\n",
        "            max_length=tgt_max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "    token_count = 1\n",
        "    while True and token_count < tgt_max_length:\n",
        "      src_token_list = src_tokens[\"input_ids\"].unsqueeze(0)\n",
        "      tgt_token_list = tgt_tokens[\"input_ids\"].unsqueeze(0)\n",
        "      att_mask = torch.cat((torch.ones(token_count), torch.zeros(tgt_token_list.shape[1] - token_count))).unsqueeze(0)\n",
        "      outputs = model(source_tokens=src_token_list, target_tokens=tgt_token_list, attention_mask=att_mask)\n",
        "      _, pred = torch.max(outputs, dim=-1)\n",
        "      next_word = CODE_VOCABULARY[pred]\n",
        "      if next_word == EOS_TOKEN:\n",
        "        break\n",
        "      translation += \" \"  + next_word\n",
        "      token_count += 1\n",
        "    return translation\n",
        "\n",
        "n_examples = 10\n",
        "data = generate_data(n_examples)\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sN-d39Qs_xV",
        "outputId": "c608250f-5d24-4830-ff0d-7ab3e212b3bc"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['shape_list', 'code_str', 'text', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for d in data:\n",
        "  print(d[\"text\"])\n",
        "  print(translate(d[\"text\"], MyTokenizer(VOCABULARY), MyRandomMaskTokenizer(CODE_VOCABULARY), MAX_LEN_SENTENCE, MAX_CODE_LEN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsgL4TBL5aWq",
        "outputId": "633c68ca-d2db-4e01-f85a-9cf50da25064"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a triangle\n",
            " 1 1 1 1 1 1 1\n",
            "one circle and a circle\n",
            " 1 1 1 1 1 1 1\n",
            "one triangle and one circle and circle after triangle\n",
            " 1 1 1 1 1 1 1\n",
            "a triangle and one triangle and one circle\n",
            " 1 1 1 1 1 1 1\n",
            "a circle\n",
            " 1 1 1 1 1 1 1\n",
            "circle after circle and a triangle\n",
            " 1 1 1 1 1 1 1\n",
            "triangle after circle then a circle\n",
            " 1 1 1 1 1 1 1\n",
            "one circle and a triangle\n",
            " 1 1 1 1 1 1 1\n",
            "one triangle then one triangle and a circle\n",
            " 1 1 1 1 1 1 1\n",
            "a circle\n",
            " 1 1 1 1 1 1 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouHaPl3T5goI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}