{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNrvfR0Ip0rvEC5KR1Yy82",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carloscotrini/transformers_from_scratch/blob/main/AML_MyTransfomerV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7GWhXikQiMT2"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "SHAPES = [\"triangle\", \"circle\"]\n",
        "PLURALS = [shape + \"s\" for shape in SHAPES]\n",
        "ARTICLES = [\"a\", \"one\"]\n",
        "TWO_ARTICLES = [\"two\"]\n",
        "CONNECTORS = [\"and\", \"then\"]\n",
        "REVERSE_CONNECTORS = [\"after\"]\n",
        "CLASS_TOKEN = \"CLS\"\n",
        "MASK_TOKEN = \"MASK\"\n",
        "SEP_TOKEN = \"SEP\"\n",
        "PAD_TOKEN = \"PAD\"\n",
        "EOS_TOKEN = \"EOS\"\n",
        "SPECIAL_TOKENS = [CLASS_TOKEN, MASK_TOKEN, SEP_TOKEN, PAD_TOKEN, EOS_TOKEN]\n",
        "VOCABULARY = SHAPES + PLURALS + ARTICLES + CONNECTORS + REVERSE_CONNECTORS + TWO_ARTICLES + SPECIAL_TOKENS\n",
        "MAX_LEN_SENTENCE = 16 # Maximum possible length of a sequence\n",
        "\n",
        "def generate_descr_from_list(r):\n",
        "  if len(r) > 4:\n",
        "    raise Exception(\"Too many items\")\n",
        "  elif len(r) == 0:\n",
        "    return \"\"\n",
        "  elif len(r) == 1:\n",
        "    article = random.choice(ARTICLES)\n",
        "    return \"{} {}\".format(article, r[0])\n",
        "  else:\n",
        "    reversed_descr = random.random() > 0.5\n",
        "    if reversed_descr:\n",
        "      descr = \"{} {} {}\".format(r[1], random.choice(REVERSE_CONNECTORS), r[0])\n",
        "      if len(r) > 2:\n",
        "        return descr + \" \" + random.choice(CONNECTORS) + \" \" + generate_descr_from_list(r[2:])\n",
        "      return descr\n",
        "    if r[0] == r[1]:\n",
        "      plural_desc = random.random() > 0.5\n",
        "      if plural_desc:\n",
        "          return \"{} {}s\".format(random.choice(TWO_ARTICLES), r[0])\n",
        "    return generate_descr_from_list([r[0]]) + \" \" + random.choice(CONNECTORS) + \" \" + generate_descr_from_list(r[1:])\n",
        "\n",
        "\n",
        "def generate_random_shapes():\n",
        "  num_shapes = random.randint(1, 4)\n",
        "  result = []\n",
        "  for _ in range(num_shapes):\n",
        "    result.append(random.choice(SHAPES))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import random\n",
        "\n",
        "def generate_image(word_list, filename):\n",
        "    # Create a blank 32x32 image\n",
        "    image_size = 32\n",
        "    patch_size = 16\n",
        "    image = Image.new(\"1\", (image_size, image_size), 1)  # '1' for 1-bit pixels, black and white\n",
        "\n",
        "    for i, word in enumerate(word_list):\n",
        "        if word not in [\"triangle\", \"circle\"]:\n",
        "            continue\n",
        "\n",
        "        # Determine the top-left corner of the patch\n",
        "        x_offset = (i % 2) * patch_size\n",
        "        y_offset = (i // 2) * patch_size\n",
        "\n",
        "        # Draw the shape in the corresponding patch\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        if word == \"triangle\":\n",
        "            points = [(random.randint(x_offset, x_offset + patch_size), random.randint(y_offset, y_offset + patch_size)) for _ in range(3)]\n",
        "            draw.polygon(points, fill=0)\n",
        "        elif word == \"circle\":\n",
        "            radius = random.randint(2, patch_size // 2)\n",
        "            center_x = random.randint(x_offset + radius, x_offset + patch_size - radius)\n",
        "            center_y = random.randint(y_offset + radius, y_offset + patch_size - radius)\n",
        "            draw.ellipse([center_x - radius, center_y - radius, center_x + radius, center_y + radius], fill=0)\n",
        "\n",
        "    # Save the image to the specified filename\n",
        "    image.save(filename)\n",
        "\n",
        "def plot_image(filename):\n",
        "    # Open the image\n",
        "    image = Image.open(filename)\n",
        "\n",
        "    # Convert the image to a NumPy array\n",
        "    image_array = np.array(image)\n",
        "\n",
        "    # Plot the image\n",
        "    plt.imshow(image_array)\n",
        "    plt.axis('off')  # Turn off axis labels\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "generate_image([\"circle\", \"triangle\", \"circle\"], \"output_image.png\")\n"
      ],
      "metadata": {
        "id": "wTQhOfTfnv11"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBER_CODES = [str(i) for i in range(5)]\n",
        "SHAPE_CODES = [shape[0] for shape in SHAPES]\n",
        "CODES = NUMBER_CODES + SHAPE_CODES\n",
        "CODE_VOCABULARY = SPECIAL_TOKENS + CODES\n",
        "\n",
        "def generate_code_str(shape_list):\n",
        "  codes = []\n",
        "  i = 0\n",
        "  while i < len(shape_list):\n",
        "    j = i + 1\n",
        "    while j < len(shape_list) and shape_list[i] == shape_list[j]:\n",
        "      j += 1\n",
        "    codes.append(f\"{NUMBER_CODES[j-i]} {shape_list[i][0]}\")\n",
        "    i = j\n",
        "  return \" \".join(codes)"
      ],
      "metadata": {
        "id": "fKuFwFcvmroH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(n_samples):\n",
        "  examples = []\n",
        "  for i in range(n_samples):\n",
        "    shape_list = generate_random_shapes()\n",
        "    code_str = generate_code_str(shape_list)\n",
        "    text = generate_descr_from_list(shape_list)\n",
        "    filename = f\"f{i}.png\"\n",
        "    generate_image(shape_list, filename)\n",
        "    examples.append({\"shape_list\": shape_list, \"code_str\": code_str, \"text\": text, \"filename\": filename})\n",
        "  return examples\n"
      ],
      "metadata": {
        "id": "VvfchFGDpLHC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "# Step 1: Prepare the Dataset\n",
        "class CountingFiguresDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "TRF1Xd44rZsx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88p-uxGSSEbs",
        "outputId": "658fdf0e-6332-47f1-fb7c-b78862f49f58"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from collections import OrderedDict\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "class MyTokenizer:\n",
        "    def __init__(self, vocabulary):\n",
        "        self.vocabulary = vocabulary\n",
        "\n",
        "    def encode(self, text, add_special_tokens=True, max_length=MAX_LEN_SENTENCE, return_token_type_ids=False, padding='max_length', return_attention_mask=True, return_tensors='pt'):\n",
        "        tokens = text.split()\n",
        "        tokens.append(EOS_TOKEN)\n",
        "        input_ids = [self.vocabulary.index(token) for token in tokens]\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        if add_special_tokens:\n",
        "            input_ids = [self.vocabulary.index(CLASS_TOKEN)] + input_ids\n",
        "            attention_mask += [1]\n",
        "\n",
        "        sen_len = len(input_ids)\n",
        "        if len(input_ids) > max_length:\n",
        "            input_ids = input_ids[:max_length]\n",
        "            attention_mask = attention_mask[:max_length]\n",
        "            sen_len = max_length\n",
        "        else:\n",
        "            pad_length = max_length - len(input_ids)\n",
        "            if pad_length >= 0:\n",
        "                input_ids += [self.vocabulary.index(PAD_TOKEN)] * pad_length\n",
        "                attention_mask += [0] * pad_length\n",
        "\n",
        "        return sen_len, input_ids, attention_mask\n",
        "\n",
        "    def encode_plus(self, text, add_special_tokens=True, max_length=MAX_LEN_SENTENCE, return_token_type_ids=False, padding='max_length', return_attention_mask=True, return_tensors='pt'):\n",
        "        _, input_ids, attention_mask = self.encode(text, add_special_tokens, max_length, return_token_type_ids, padding, return_attention_mask, return_tensors)\n",
        "        if return_attention_mask:\n",
        "          return {\n",
        "              'input_ids': torch.tensor(input_ids),\n",
        "              'attention_mask': torch.tensor(attention_mask)\n",
        "          }\n",
        "        else:\n",
        "          return {\n",
        "              'input_ids': torch.tensor(input_ids)\n",
        "          }\n"
      ],
      "metadata": {
        "id": "S1ey-4OGDwWF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_key_dim, hidden_val_dim, enc_emb_dim=None, num_heads=1):\n",
        "        \"\"\"\n",
        "          Implements an attention mechanism\n",
        "\n",
        "          Args:\n",
        "          input_dim: Dimensionality of input embedding.\n",
        "          hidden_key_dim: Dimensionality of key and query vectors.\n",
        "          hidden_val_dim: Dimensionality of value vectors.\n",
        "          enc_emb_dim: Dimensionality of encoder embeddings. If None, self-attention is used.\n",
        "          mask: Whether to apply masking. If True, the attention scores for masked positions are set to -inf.\n",
        "          num_heads: Number of attention heads.\n",
        "        \"\"\"\n",
        "        super(MyAttention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.enc_emb_dim = enc_emb_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.to_q = nn.Linear(self.input_dim, self.hidden_key_dim * self.num_heads, bias=False)\n",
        "\n",
        "        for i in range(self.num_heads):\n",
        "          if enc_emb_dim is None:\n",
        "              self.to_k = nn.Linear(self.input_dim, self.hidden_key_dim * self.num_heads, bias=False)\n",
        "              self.to_v = nn.Linear(self.input_dim, self.hidden_val_dim * self.num_heads, bias=False)\n",
        "          else:\n",
        "              self.to_k = nn.Linear(self.enc_emb_dim, self.hidden_key_dim * self.num_heads, bias=False)\n",
        "              self.to_v = nn.Linear(self.enc_emb_dim, self.hidden_val_dim * self.num_heads, bias=False)\n",
        "\n",
        "        self.to_out = nn.Linear(self.hidden_val_dim * self.num_heads, self.input_dim)\n",
        "\n",
        "    def forward(self, embeddings, encoder_embeddings=None, attention_mask=None):\n",
        "\n",
        "        if encoder_embeddings is not None and attention_mask is not None:\n",
        "            raise Exception(\"In cross attention there is no masking.\")\n",
        "\n",
        "        if encoder_embeddings is None:\n",
        "            Q = self.to_q(embeddings)\n",
        "            K = self.to_k(embeddings)\n",
        "            V = self.to_v(embeddings)\n",
        "        else:\n",
        "            Q = self.to_q(embeddings)\n",
        "            K = self.to_k(encoder_embeddings)\n",
        "            V = self.to_v(encoder_embeddings)\n",
        "\n",
        "        Q = rearrange(Q, 'B T (H D) -> B H T D', H=self.num_heads, D=self.hidden_key_dim)\n",
        "        K = rearrange(K, 'B T (H D) -> B H T D', H=self.num_heads, D=self.hidden_key_dim)\n",
        "        V = rearrange(V, 'B T (H D) -> B H T D', H=self.num_heads, D=self.hidden_val_dim)\n",
        "\n",
        "        scores = torch.einsum(\"BHTD,BHSD->BHTS\", Q, K)\n",
        "\n",
        "        if attention_mask is not None and encoder_embeddings is None:\n",
        "            # Originally, attention_mask has shape (batch_size, sequence_len)\n",
        "            # To ensure propagation to the scores matrix, which has shape (batch_size, num_heads, sequence_len, sequence_len),\n",
        "            # We need to make attention_mask's shape (batch_size, 1, 1, sequence_len)\n",
        "            # We do this with the unsqueeze method, which adds a new dimension.\n",
        "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
        "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
        "\n",
        "        attnmats = F.softmax(scores / math.sqrt(self.hidden_key_dim), dim=-1)\n",
        "\n",
        "        ctx_vecs = torch.einsum(\"BHTS,BHSD->BHTD\", attnmats, V)\n",
        "        ctx_vecs = rearrange(ctx_vecs, 'B H T D -> B T (H D)', H=self.num_heads, D=self.hidden_val_dim)\n",
        "        return self.to_out(ctx_vecs)"
      ],
      "metadata": {
        "id": "aFYriqViEvVt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=MAX_LEN_SENTENCE):\n",
        "        super(MyPositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a matrix of shape (max_len, d_model) with all zeros\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Create a column vector of shape (max_len, 1) with values [0, 1, ..., max_len-1]\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Create a row vector of shape (1, d_model // 2) with values [0, 1, ..., d_model//2-1]\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even indices and cosine to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add a batch dimension (1, max_len, d_model) and register as buffer\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input tensor (B, T, D)\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "HJBjsyuMMeLO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_key_dim, hidden_val_dim, output_dim, num_heads=1):\n",
        "        super(MyTransformerEncoderLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.attention = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=None, num_heads=self.num_heads)\n",
        "        self.norm1 = nn.LayerNorm(self.input_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, self.output_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(self.output_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        x = self.norm1(self.attention(x, attention_mask=attention_mask) + x)\n",
        "        x = self.norm2(self.feed_forward(x) + x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jt6_PYhSNLBh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_tokens, input_dim, hidden_key_dim, hidden_val_dim, output_dim, max_length, num_layers=1, num_heads=1):\n",
        "        super(MyTransformerEncoder, self).__init__()\n",
        "        self.num_tokens = num_tokens\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.max_length = max_length\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.embedding = nn.Embedding(num_tokens, self.input_dim)\n",
        "        self.positional_encoding = MyPositionalEncoding(self.input_dim, max_length)\n",
        "        self.layers = nn.ModuleList([MyTransformerEncoderLayer(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, self.input_dim, num_heads) for _ in range(self.num_layers)])\n",
        "        self.linear = nn.Linear(self.input_dim, self.output_dim)\n",
        "        self.norm = nn.LayerNorm(self.output_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask=attention_mask)\n",
        "        x = self.linear(x)\n",
        "        x = self.norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "N90NAhGjMgZu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerClassifier(nn.Module):\n",
        "    def __init__(self, transf_enc, num_classes):\n",
        "        super(MyTransformerClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.transf_enc = transf_enc\n",
        "\n",
        "        self.linear = nn.Linear(self.transf_enc.output_dim, self.num_classes)\n",
        "\n",
        "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
        "        x = self.transf_enc(input_ids, attention_mask)[:, 0, :] # Just the embedding of the first token, which is the CLS token.\n",
        "        logits = self.linear(x)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss = criterion(logits, labels)\n",
        "        return (loss, logits) if loss is not None else logits"
      ],
      "metadata": {
        "id": "thRPUXH4PtXH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "\n",
        "# Step 2: Tokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer = MyTokenizer(VOCABULARY)\n",
        "tokenizer.encode_plus(\n",
        "            \"one circle after one circle and two triangles\",\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN_SENTENCE,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUw3D9j26rgn",
        "outputId": "10f7971d-03b6-4da5-e7fe-7e06d5ecba67"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([10,  5,  1,  8,  5,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 100\n",
        "data = generate_data(n_samples)\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dlKZJQsCwew",
        "outputId": "8fcc484a-bd94-4397-dc5f-b0bcb72437c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['shape_list', 'code_str', 'text', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset\n",
        "descriptions = [d[\"text\"] for d in data]\n",
        "lengths = [len(d[\"shape_list\"]) for d in data]\n",
        "dataset = CountingFiguresDataset(descriptions, lengths, tokenizer, max_length=MAX_LEN_SENTENCE)"
      ],
      "metadata": {
        "id": "kgwzsdVxJIdL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(VOCABULARY)\n",
        "input_dim = 16\n",
        "hidden_key_dim = 8\n",
        "hidden_val_dim = 8\n",
        "num_heads = 2\n",
        "output_dim = 16\n",
        "num_layers = 3\n",
        "num_labels = 5\n",
        "\n",
        "# Step 3: Model\n",
        "transf_enc = MyTransformerEncoder(num_tokens, input_dim=input_dim, hidden_key_dim=hidden_key_dim, hidden_val_dim=hidden_val_dim, output_dim=output_dim, max_length=MAX_LEN_SENTENCE, num_layers=num_layers, num_heads=num_heads)\n",
        "model = MyTransformerClassifier(transf_enc, num_labels)\n",
        "\n",
        "# Step 4: Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=200,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.001,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Split dataset into train and eval\n",
        "train_size = int(0.8 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
        "\n",
        "# Custom Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "DQbq5O20F1Nx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Step 5: Evaluation\n",
        "def evaluate_model(texts, labels):\n",
        "    eval_dataset = CountingFiguresDataset(texts, labels, tokenizer, max_length=MAX_LEN_SENTENCE)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=2)\n",
        "    total_correct = 0\n",
        "    total_samples = len(labels)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=-1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "# Generate descriptions and images for each shape list\n",
        "\n",
        "test_shape_lists = [generate_random_shapes() for _ in range(100)]\n",
        "\n",
        "eval_descriptions = []\n",
        "eval_lengths = []\n",
        "for i, shape_list in enumerate(test_shape_lists):\n",
        "  eval_descriptions.append(generate_descr_from_list(shape_list))\n",
        "  eval_lengths.append(len(shape_list))\n",
        "\n",
        "evaluate_model(eval_descriptions, eval_lengths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4j1RRLb7hzX",
        "outputId": "bfdd3851-c34b-477c-bebe-12f76161431b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 42.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_key_dim, hidden_val_dim, output_dim, enc_emb_dim, num_heads):\n",
        "        super(MyTransformerDecoderLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.enc_emb_dim = enc_emb_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.masked_att = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=None, num_heads=self.num_heads)\n",
        "        self.norm1 = nn.LayerNorm(self.input_dim)\n",
        "        self.cross_att = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=self.enc_emb_dim, num_heads=self.num_heads)\n",
        "        self.norm2 = nn.LayerNorm(self.input_dim)\n",
        "        self.self_att = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=None, num_heads=self.num_heads)\n",
        "        self.norm3 = nn.LayerNorm(self.input_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, self.output_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, enc_emb, attention_mask):\n",
        "        x = self.norm1(x + self.masked_att(x, attention_mask=attention_mask))\n",
        "        x = self.norm2(x + self.cross_att(x, encoder_embeddings=enc_emb))\n",
        "        x = self.norm3(x + self.self_att(x, attention_mask=attention_mask))\n",
        "        x = self.feed_forward(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jeRheKQGyfmO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_tokens, input_dim, hidden_key_dim,\n",
        "                 hidden_val_dim, output_dim, transf_enc,\n",
        "                 max_length, num_layers, num_heads):\n",
        "        super(MyTransformerDecoder, self).__init__()\n",
        "        self.num_tokens = num_tokens\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.max_length = max_length\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.transf_enc = transf_enc\n",
        "\n",
        "        if self.transf_enc.num_layers != self.num_layers:\n",
        "            raise Exception(\"The number of layers in the encoder and decoder must be the same.\")\n",
        "\n",
        "        self.embedding = nn.Embedding(num_tokens, self.input_dim)\n",
        "        self.positional_encoding = MyPositionalEncoding(self.input_dim, max_length)\n",
        "        self.layers = nn.ModuleList([MyTransformerDecoderLayer(input_dim=self.input_dim,\n",
        "                                                               hidden_key_dim=self.hidden_key_dim,\n",
        "                                                               hidden_val_dim=self.hidden_val_dim,\n",
        "                                                               enc_emb_dim=self.transf_enc.input_dim,\n",
        "                                                               output_dim=self.output_dim,\n",
        "                                                               num_heads=num_heads) for _ in range(self.num_layers)])\n",
        "        self.linear = nn.Linear(self.input_dim, self.output_dim)\n",
        "        self.norm = nn.LayerNorm(self.output_dim)\n",
        "\n",
        "    def forward(self, x, x_enc, attention_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x_enc = self.transf_enc.embedding(x_enc)\n",
        "        x_enc = self.transf_enc.positional_encoding(x_enc)\n",
        "        for i in range(self.num_layers):\n",
        "            x_enc = self.transf_enc.layers[i](x_enc)\n",
        "            x = self.layers[i](x, x_enc, attention_mask=attention_mask)\n",
        "        x = self.linear(x)\n",
        "        x = self.norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LON48B-h4zj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerTranslator(nn.Module):\n",
        "    def __init__(self, transf_dec, num_tokens_target_vocab):\n",
        "        super(MyTransformerTranslator, self).__init__()\n",
        "        self.transf_dec = transf_dec\n",
        "        self.linear = nn.Linear(self.transf_dec.output_dim, num_tokens_target_vocab)\n",
        "\n",
        "    def forward(self, source_tokens, target_tokens, attention_mask, labels=None):\n",
        "        decoded_embeddings = self.transf_dec(target_tokens, source_tokens, attention_mask)[:, 0, :]\n",
        "        logits = self.linear(decoded_embeddings)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss = criterion(logits, labels)\n",
        "        return (loss, logits) if loss is not None else logits"
      ],
      "metadata": {
        "id": "LLudUsAtwhAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MyRandomMaskTokenizer(MyTokenizer):\n",
        "  def __init__(self, vocabulary):\n",
        "    super().__init__(vocabulary)\n",
        "\n",
        "  def encode_plus(self, text, add_special_tokens=True, max_length=MAX_LEN_SENTENCE, return_token_type_ids=False, padding='max_length', return_attention_mask=True, return_tensors='pt'):\n",
        "    sen_len, input_ids, attention_mask = super().encode(text, add_special_tokens, max_length, return_token_type_ids, padding, return_attention_mask, return_tensors)\n",
        "    if return_attention_mask:\n",
        "      new_len = random.randint(1, sen_len-1)\n",
        "      new_attention_mask = torch.cat((torch.ones(new_len), torch.zeros(len(input_ids) - new_len)))\n",
        "      return {\n",
        "              'input_ids': torch.tensor(input_ids),\n",
        "              'attention_mask': new_attention_mask,\n",
        "              'sen_len': new_len\n",
        "      }\n",
        "    else:\n",
        "      return {\n",
        "              'input_ids': torch.tensor(input_ids),\n",
        "              'sen_len': new_len\n",
        "      }\n",
        "\n",
        "tokenizer = MyRandomMaskTokenizer(CODE_VOCABULARY)\n",
        "tokenizer.encode_plus(\n",
        "            \"c 2 t 2\",\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN_SENTENCE,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvU4FiMZzYUU",
        "outputId": "bd7e1421-7b42-43e4-a0c0-ad05477ce015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 0, 11,  7, 10,  7,  4,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3]),\n",
              " 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'sen_len': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_texts, tgt_texts, src_tokenizer, tgt_tokenizer, src_max_len, tgt_max_len):\n",
        "        self.src_texts = src_texts\n",
        "        self.tgt_texts = tgt_texts\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.src_max_len = src_max_len\n",
        "        self.tgt_max_len = tgt_max_len\n",
        "\n",
        "        self.tgt_encodings = [self.tgt_tokenizer.encode_plus(\n",
        "              txt,\n",
        "              add_special_tokens=True,\n",
        "              max_length=self.tgt_max_len,\n",
        "              return_token_type_ids=False,\n",
        "              padding='max_length',\n",
        "              return_attention_mask=True,\n",
        "              return_tensors='pt'\n",
        "            ) for txt in self.tgt_texts]\n",
        "\n",
        "        self.labels=[]\n",
        "        for tgt_encoding in self.tgt_encodings:\n",
        "          input_ids = tgt_encoding['input_ids']\n",
        "          label = input_ids[tgt_encoding['sen_len']]\n",
        "          self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_encoding = self.src_tokenizer.encode_plus(\n",
        "            self.src_texts[idx],\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.src_max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'source_tokens': src_encoding['input_ids'].flatten(),\n",
        "            'target_tokens': self.tgt_encodings[idx]['input_ids'].flatten(),\n",
        "            'attention_mask': self.tgt_encodings[idx]['attention_mask'].flatten(),\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "zgAr-duaEWP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CODE_LEN=8\n",
        "\n",
        "n_examples = 400\n",
        "data = generate_data(n_examples)\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPNYvGpfUVI9",
        "outputId": "8a48baec-8f41-40e3-f6d0-95af178e4510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['shape_list', 'code_str', 'text', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "descriptions = [d[\"text\"] for d in data]\n",
        "code_lists = [d[\"code_str\"] for d in data]"
      ],
      "metadata": {
        "id": "ujIdHu501lA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transl_dataset = TranslationDataset(descriptions, code_lists, MyTokenizer(VOCABULARY), MyRandomMaskTokenizer(CODE_VOCABULARY), MAX_LEN_SENTENCE, tgt_max_len=MAX_CODE_LEN)\n",
        "for example in transl_dataset:\n",
        "  print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4OF45l51jYX",
        "outputId": "0eec6818-f9d1-4230-f4ee-d476a2c09bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source_tokens': tensor([10,  4,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  5,  1,  7,  1,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  0,  7,  5,  1,  6,  4,  0, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  4,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  0,  6,  5,  0,  7,  4,  0, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  8, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  4,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  5,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  5,  1,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  0,  8,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  5,  0,  7,  4,  0,  7,  5,  1, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  5,  1,  7,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  0,  8,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  5,  0,  6,  0,  8,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  4,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  8, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  4,  0,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  5,  0,  7,  4,  0,  7,  5,  1, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  5,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  4,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  1,  8,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  8, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  5,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  0,  8,  0,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  8, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  4,  0,  7,  9,  3, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  4,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  5,  1,  6,  5,  0,  6,  4,  0, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  0,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  0,  8,  0,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  5,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  0,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  4,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  5,  0,  7,  1,  8,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  5,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  5,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  7, 11,  4]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  0,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  4,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  4,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  4,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  7, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  0,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  5,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  5,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  7, 11,  4]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  5,  1,  6,  5,  0,  7,  5,  1, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  5,  1,  7,  5,  0,  6,  5,  1, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  6,  4,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  7, 11,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  6,  4,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  5,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  6,  5,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  4,  0,  6,  1,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  7, 11,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  0,  8,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  4,  1,  6,  4,  0,  7,  5,  0, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  4,  0,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  1,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  7, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  5,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  5,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  5,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  0,  8,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  4,  0,  7,  5,  1,  6,  5,  0, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  4,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  5,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  4,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0,  6,  5,  1,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  4,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1,  6,  4,  0,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  0,  8,  1,  7,  5,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  5,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1,  7,  4,  0,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1,  6,  1,  8,  0,  6,  4,  0, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  6,  4,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  0,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  0,  6,  5,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  5,  0,  7,  1,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  1,  7,  5,  1,  6,  4,  1,  7,  4,  1, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_tokens': tensor([10,  4,  0,  7,  5,  1,  7,  4,  0,  7,  4,  1, 14, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  1,  8,  1,  7,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_tokens': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  1,  8,  0,  7,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_tokens': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_tokens': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_tokens': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_tokens': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_src_tokens=len(VOCABULARY)\n",
        "num_tgt_tokens=len(CODE_VOCABULARY)\n",
        "input_dim = 8\n",
        "hidden_key_dim = 8\n",
        "hidden_val_dim = 8\n",
        "num_heads = 3\n",
        "output_dim = 8\n",
        "num_layers = 4\n",
        "\n",
        "transf_enc = MyTransformerEncoder(num_src_tokens, input_dim=input_dim, hidden_key_dim=hidden_key_dim, hidden_val_dim=hidden_val_dim, output_dim=output_dim, max_length=MAX_LEN_SENTENCE, num_layers=num_layers, num_heads=num_heads)\n",
        "transf_dec = MyTransformerDecoder(num_tgt_tokens, input_dim=input_dim, hidden_key_dim=hidden_key_dim, hidden_val_dim=hidden_val_dim, output_dim=output_dim, transf_enc=transf_enc, max_length=MAX_CODE_LEN, num_layers=num_layers, num_heads=num_heads)\n",
        "model = MyTransformerTranslator(transf_dec, len(CODE_VOCABULARY))\n",
        "\n",
        "# Step 4: Training\n",
        "num_epochs = 400\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.001,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Split dataset into train and eval\n",
        "train_size = int(0.8 * len(transl_dataset))\n",
        "eval_size = len(transl_dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(transl_dataset, [train_size, eval_size])\n",
        "\n",
        "# Custom Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MGMACM7bVOnt",
        "outputId": "82298dce-8223-4e2f-f77c-faae1ccd8da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64000' max='64000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64000/64000 50:58, Epoch 400/400]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.675800</td>\n",
              "      <td>2.631782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.476700</td>\n",
              "      <td>2.528121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.468700</td>\n",
              "      <td>2.467000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.480700</td>\n",
              "      <td>2.417998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.448900</td>\n",
              "      <td>2.374194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.364800</td>\n",
              "      <td>2.332514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.313800</td>\n",
              "      <td>2.292840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.199800</td>\n",
              "      <td>2.259204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.255200</td>\n",
              "      <td>2.227817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.250700</td>\n",
              "      <td>2.195354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.249700</td>\n",
              "      <td>2.164759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.156200</td>\n",
              "      <td>2.137298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.060900</td>\n",
              "      <td>2.120073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.002500</td>\n",
              "      <td>2.095735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.111000</td>\n",
              "      <td>2.070606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.116100</td>\n",
              "      <td>2.040149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.933500</td>\n",
              "      <td>2.024023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.019300</td>\n",
              "      <td>1.991159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.916900</td>\n",
              "      <td>1.960150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.831900</td>\n",
              "      <td>1.941873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.706700</td>\n",
              "      <td>1.914443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.798900</td>\n",
              "      <td>1.886068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.872300</td>\n",
              "      <td>1.860638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.875600</td>\n",
              "      <td>1.834359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.760600</td>\n",
              "      <td>1.821437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.758000</td>\n",
              "      <td>1.794984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.737700</td>\n",
              "      <td>1.778749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.764000</td>\n",
              "      <td>1.747818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.691800</td>\n",
              "      <td>1.733698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.612400</td>\n",
              "      <td>1.712211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.692400</td>\n",
              "      <td>1.697238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.598300</td>\n",
              "      <td>1.696865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.564100</td>\n",
              "      <td>1.678815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.654000</td>\n",
              "      <td>1.656273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.487500</td>\n",
              "      <td>1.646548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.387400</td>\n",
              "      <td>1.634493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.716500</td>\n",
              "      <td>1.599452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.448600</td>\n",
              "      <td>1.568179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.531200</td>\n",
              "      <td>1.580704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.440800</td>\n",
              "      <td>1.564295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.447600</td>\n",
              "      <td>1.554203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.312400</td>\n",
              "      <td>1.539539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.382200</td>\n",
              "      <td>1.499143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.561500</td>\n",
              "      <td>1.507366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.335100</td>\n",
              "      <td>1.471045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.458800</td>\n",
              "      <td>1.441286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.441000</td>\n",
              "      <td>1.457302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.119100</td>\n",
              "      <td>1.436072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.373100</td>\n",
              "      <td>1.428006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.387900</td>\n",
              "      <td>1.413713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.356500</td>\n",
              "      <td>1.388420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.300000</td>\n",
              "      <td>1.380492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.326700</td>\n",
              "      <td>1.355385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.323600</td>\n",
              "      <td>1.379663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.226400</td>\n",
              "      <td>1.373964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.276000</td>\n",
              "      <td>1.336294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.239700</td>\n",
              "      <td>1.326726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.103200</td>\n",
              "      <td>1.295087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.155700</td>\n",
              "      <td>1.279966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.105900</td>\n",
              "      <td>1.282972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.867800</td>\n",
              "      <td>1.273635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.208300</td>\n",
              "      <td>1.265303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.153900</td>\n",
              "      <td>1.254583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.286800</td>\n",
              "      <td>1.223657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.917500</td>\n",
              "      <td>1.213213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.037100</td>\n",
              "      <td>1.187185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.907700</td>\n",
              "      <td>1.218821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.913600</td>\n",
              "      <td>1.173504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.979400</td>\n",
              "      <td>1.145908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.961700</td>\n",
              "      <td>1.167464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.193100</td>\n",
              "      <td>1.149833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.166100</td>\n",
              "      <td>1.141024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.116100</td>\n",
              "      <td>1.128994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.899900</td>\n",
              "      <td>1.127089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.981100</td>\n",
              "      <td>1.110205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.000200</td>\n",
              "      <td>1.089374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.825500</td>\n",
              "      <td>1.073577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.894100</td>\n",
              "      <td>1.061419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.002900</td>\n",
              "      <td>1.064504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.116800</td>\n",
              "      <td>1.036342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.950600</td>\n",
              "      <td>1.025010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.933000</td>\n",
              "      <td>1.006237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.940900</td>\n",
              "      <td>0.986904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.019400</td>\n",
              "      <td>0.980748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.041300</td>\n",
              "      <td>0.978497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.930600</td>\n",
              "      <td>0.968542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>0.962446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.985900</td>\n",
              "      <td>0.945103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.963600</td>\n",
              "      <td>0.943419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.113500</td>\n",
              "      <td>0.935421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.143500</td>\n",
              "      <td>0.948562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.663500</td>\n",
              "      <td>0.911941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.744900</td>\n",
              "      <td>0.930336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.522300</td>\n",
              "      <td>0.924196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.979300</td>\n",
              "      <td>0.930598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.678800</td>\n",
              "      <td>0.909881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.856000</td>\n",
              "      <td>0.903281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.829600</td>\n",
              "      <td>0.903958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.026500</td>\n",
              "      <td>0.899103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.961300</td>\n",
              "      <td>0.887453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.667600</td>\n",
              "      <td>0.893766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.975500</td>\n",
              "      <td>0.864510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.985300</td>\n",
              "      <td>0.922814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.920200</td>\n",
              "      <td>0.878256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.866295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.845300</td>\n",
              "      <td>0.867542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.575600</td>\n",
              "      <td>0.860324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>1.022100</td>\n",
              "      <td>0.826094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.989200</td>\n",
              "      <td>0.817106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.943700</td>\n",
              "      <td>0.840113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.831500</td>\n",
              "      <td>0.834882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.869011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.985700</td>\n",
              "      <td>0.851026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.786100</td>\n",
              "      <td>0.801069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.452500</td>\n",
              "      <td>0.820464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.963200</td>\n",
              "      <td>0.799965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.828600</td>\n",
              "      <td>0.840224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.923300</td>\n",
              "      <td>0.795793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.927400</td>\n",
              "      <td>0.803210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.188900</td>\n",
              "      <td>0.788600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.745600</td>\n",
              "      <td>0.786608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.642400</td>\n",
              "      <td>0.794946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.688900</td>\n",
              "      <td>0.784369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.741700</td>\n",
              "      <td>0.772144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.341900</td>\n",
              "      <td>0.769014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.270100</td>\n",
              "      <td>0.776214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.648500</td>\n",
              "      <td>0.790498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.674500</td>\n",
              "      <td>0.757497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.657100</td>\n",
              "      <td>0.748075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.902000</td>\n",
              "      <td>0.741870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.602100</td>\n",
              "      <td>0.761778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.759500</td>\n",
              "      <td>0.778460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.647000</td>\n",
              "      <td>0.771611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>1.165900</td>\n",
              "      <td>0.764819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.990500</td>\n",
              "      <td>0.732566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.718400</td>\n",
              "      <td>0.775947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.286200</td>\n",
              "      <td>0.744470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.784900</td>\n",
              "      <td>0.756900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.674200</td>\n",
              "      <td>0.742146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.816100</td>\n",
              "      <td>0.737905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>1.020000</td>\n",
              "      <td>0.737164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.659900</td>\n",
              "      <td>0.725359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.682600</td>\n",
              "      <td>0.725187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.423000</td>\n",
              "      <td>0.711325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.681900</td>\n",
              "      <td>0.688595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.590500</td>\n",
              "      <td>0.712467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.660600</td>\n",
              "      <td>0.751634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.544500</td>\n",
              "      <td>0.714535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.508700</td>\n",
              "      <td>0.706197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.743700</td>\n",
              "      <td>0.704105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.579900</td>\n",
              "      <td>0.690785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.356300</td>\n",
              "      <td>0.693696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.479000</td>\n",
              "      <td>0.720818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>1.170900</td>\n",
              "      <td>0.693570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.510900</td>\n",
              "      <td>0.711202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.592200</td>\n",
              "      <td>0.689636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.601600</td>\n",
              "      <td>0.697623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.419000</td>\n",
              "      <td>0.679922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.906100</td>\n",
              "      <td>0.696503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.569800</td>\n",
              "      <td>0.688489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.749000</td>\n",
              "      <td>0.700607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.793300</td>\n",
              "      <td>0.657603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.515600</td>\n",
              "      <td>0.663269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.719400</td>\n",
              "      <td>0.699871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.849100</td>\n",
              "      <td>0.704497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.607600</td>\n",
              "      <td>0.696968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.381000</td>\n",
              "      <td>0.678395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.732700</td>\n",
              "      <td>0.669747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>1.058800</td>\n",
              "      <td>0.719165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.385200</td>\n",
              "      <td>0.654362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>1.094500</td>\n",
              "      <td>0.667992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.715900</td>\n",
              "      <td>0.691411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>1.023900</td>\n",
              "      <td>0.658415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>1.026000</td>\n",
              "      <td>0.626071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.674771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.755900</td>\n",
              "      <td>0.665067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.344500</td>\n",
              "      <td>0.666719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.500500</td>\n",
              "      <td>0.669256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.343800</td>\n",
              "      <td>0.671628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.970300</td>\n",
              "      <td>0.655043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>1.039800</td>\n",
              "      <td>0.647116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.449000</td>\n",
              "      <td>0.661651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.938900</td>\n",
              "      <td>0.634659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.444500</td>\n",
              "      <td>0.640342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.530300</td>\n",
              "      <td>0.636680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.709500</td>\n",
              "      <td>0.656769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.451000</td>\n",
              "      <td>0.631572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.623100</td>\n",
              "      <td>0.630311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.293900</td>\n",
              "      <td>0.635451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.516500</td>\n",
              "      <td>0.650605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.769400</td>\n",
              "      <td>0.626005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.502200</td>\n",
              "      <td>0.661165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.787000</td>\n",
              "      <td>0.637401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.664700</td>\n",
              "      <td>0.647100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.562900</td>\n",
              "      <td>0.619058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.610500</td>\n",
              "      <td>0.619021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.824300</td>\n",
              "      <td>0.691750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.764800</td>\n",
              "      <td>0.653984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.311700</td>\n",
              "      <td>0.631688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.881800</td>\n",
              "      <td>0.631373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.678300</td>\n",
              "      <td>0.615640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.896700</td>\n",
              "      <td>0.676896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.685400</td>\n",
              "      <td>0.623708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.569000</td>\n",
              "      <td>0.601701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.582100</td>\n",
              "      <td>0.608917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.614430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>1.261400</td>\n",
              "      <td>0.624453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.624900</td>\n",
              "      <td>0.600883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.546600</td>\n",
              "      <td>0.613182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.438700</td>\n",
              "      <td>0.617087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.335500</td>\n",
              "      <td>0.593172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.350800</td>\n",
              "      <td>0.604644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.561100</td>\n",
              "      <td>0.596807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.419900</td>\n",
              "      <td>0.602927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.722400</td>\n",
              "      <td>0.601680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.545500</td>\n",
              "      <td>0.656641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.760900</td>\n",
              "      <td>0.578079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>0.084300</td>\n",
              "      <td>0.592521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.592300</td>\n",
              "      <td>0.602618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.825900</td>\n",
              "      <td>0.590960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.379600</td>\n",
              "      <td>0.584140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>1.048800</td>\n",
              "      <td>0.577793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.596915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.549200</td>\n",
              "      <td>0.603492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.459200</td>\n",
              "      <td>0.583679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.470500</td>\n",
              "      <td>0.586933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.270200</td>\n",
              "      <td>0.611264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.571400</td>\n",
              "      <td>0.580277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.484700</td>\n",
              "      <td>0.562273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.384200</td>\n",
              "      <td>0.558007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.373900</td>\n",
              "      <td>0.570556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.246100</td>\n",
              "      <td>0.548073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>0.643200</td>\n",
              "      <td>0.590939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.750600</td>\n",
              "      <td>0.589730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.613872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.256900</td>\n",
              "      <td>0.591777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.640500</td>\n",
              "      <td>0.546904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.514200</td>\n",
              "      <td>0.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.312300</td>\n",
              "      <td>0.584729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.413600</td>\n",
              "      <td>0.541669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.382400</td>\n",
              "      <td>0.555519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.878400</td>\n",
              "      <td>0.560744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.474100</td>\n",
              "      <td>0.565038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.564538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.471900</td>\n",
              "      <td>0.573124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.622900</td>\n",
              "      <td>0.570507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.508200</td>\n",
              "      <td>0.555642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.553530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.260400</td>\n",
              "      <td>0.553701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.655600</td>\n",
              "      <td>0.551151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>0.609600</td>\n",
              "      <td>0.537150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.435500</td>\n",
              "      <td>0.545037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>0.858600</td>\n",
              "      <td>0.552760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.240500</td>\n",
              "      <td>0.550654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.390700</td>\n",
              "      <td>0.556038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.474300</td>\n",
              "      <td>0.586319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.964900</td>\n",
              "      <td>0.540912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.366700</td>\n",
              "      <td>0.537244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.433100</td>\n",
              "      <td>0.557090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.270200</td>\n",
              "      <td>0.526939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>0.417200</td>\n",
              "      <td>0.531521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.523500</td>\n",
              "      <td>0.527428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.809400</td>\n",
              "      <td>0.527499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.432600</td>\n",
              "      <td>0.550144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.686800</td>\n",
              "      <td>0.534183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.441700</td>\n",
              "      <td>0.509260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>0.730200</td>\n",
              "      <td>0.506848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.171800</td>\n",
              "      <td>0.553769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>0.585200</td>\n",
              "      <td>0.528615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.524200</td>\n",
              "      <td>0.539021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.638500</td>\n",
              "      <td>0.529799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.387500</td>\n",
              "      <td>0.579008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>0.657700</td>\n",
              "      <td>0.520793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.573700</td>\n",
              "      <td>0.547888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.269100</td>\n",
              "      <td>0.539602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.172100</td>\n",
              "      <td>0.527201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.403900</td>\n",
              "      <td>0.525489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.215400</td>\n",
              "      <td>0.543945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>0.400200</td>\n",
              "      <td>0.512704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.233400</td>\n",
              "      <td>0.512819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.470400</td>\n",
              "      <td>0.545826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.560900</td>\n",
              "      <td>0.517430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.723000</td>\n",
              "      <td>0.532840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.040600</td>\n",
              "      <td>0.511637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.924200</td>\n",
              "      <td>0.514260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.382700</td>\n",
              "      <td>0.538203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>0.524100</td>\n",
              "      <td>0.540098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.642000</td>\n",
              "      <td>0.508206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>0.382500</td>\n",
              "      <td>0.505147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.379500</td>\n",
              "      <td>0.517876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.581400</td>\n",
              "      <td>0.535087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.547272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.364700</td>\n",
              "      <td>0.533499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.639000</td>\n",
              "      <td>0.514214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.539300</td>\n",
              "      <td>0.506961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.201600</td>\n",
              "      <td>0.533776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.543900</td>\n",
              "      <td>0.536293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.813300</td>\n",
              "      <td>0.510001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.484900</td>\n",
              "      <td>0.520449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.269700</td>\n",
              "      <td>0.511597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>0.241900</td>\n",
              "      <td>0.522591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.289900</td>\n",
              "      <td>0.522726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>0.196300</td>\n",
              "      <td>0.510783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.308600</td>\n",
              "      <td>0.519028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>0.498600</td>\n",
              "      <td>0.515362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>0.219200</td>\n",
              "      <td>0.511413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>0.379300</td>\n",
              "      <td>0.494855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.307400</td>\n",
              "      <td>0.500599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>0.480400</td>\n",
              "      <td>0.515522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.165800</td>\n",
              "      <td>0.511930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>0.304700</td>\n",
              "      <td>0.498201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.589100</td>\n",
              "      <td>0.504691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>0.186200</td>\n",
              "      <td>0.533570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>0.866700</td>\n",
              "      <td>0.488851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.984400</td>\n",
              "      <td>0.518923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.271700</td>\n",
              "      <td>0.519876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>0.419100</td>\n",
              "      <td>0.559135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>0.641700</td>\n",
              "      <td>0.541718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.526405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.111200</td>\n",
              "      <td>0.517729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>0.475600</td>\n",
              "      <td>0.508592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.357000</td>\n",
              "      <td>0.497608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>0.844200</td>\n",
              "      <td>0.525854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.399100</td>\n",
              "      <td>0.503308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.389800</td>\n",
              "      <td>0.501281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>0.098800</td>\n",
              "      <td>0.509615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>0.707200</td>\n",
              "      <td>0.503782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.274400</td>\n",
              "      <td>0.511081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>0.277400</td>\n",
              "      <td>0.537593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.181100</td>\n",
              "      <td>0.507334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>0.292700</td>\n",
              "      <td>0.496110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.397000</td>\n",
              "      <td>0.482967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>0.212000</td>\n",
              "      <td>0.492991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>0.235000</td>\n",
              "      <td>0.495972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.559000</td>\n",
              "      <td>0.514136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.324500</td>\n",
              "      <td>0.507645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>0.247900</td>\n",
              "      <td>0.519146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.492600</td>\n",
              "      <td>0.506646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>0.422000</td>\n",
              "      <td>0.511888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.244400</td>\n",
              "      <td>0.509642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>0.200900</td>\n",
              "      <td>0.511558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.239200</td>\n",
              "      <td>0.533579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>0.176600</td>\n",
              "      <td>0.496658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.333800</td>\n",
              "      <td>0.514429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.422600</td>\n",
              "      <td>0.514909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.266200</td>\n",
              "      <td>0.513104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>0.514500</td>\n",
              "      <td>0.499307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>0.134000</td>\n",
              "      <td>0.520938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>0.378200</td>\n",
              "      <td>0.513811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.632900</td>\n",
              "      <td>0.504850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>0.319200</td>\n",
              "      <td>0.509541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.503466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>0.536500</td>\n",
              "      <td>0.503359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.209200</td>\n",
              "      <td>0.501722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>0.125300</td>\n",
              "      <td>0.489474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.230600</td>\n",
              "      <td>0.518254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>0.238000</td>\n",
              "      <td>0.517820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.512100</td>\n",
              "      <td>0.509457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>0.582300</td>\n",
              "      <td>0.499694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.424600</td>\n",
              "      <td>0.505803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>0.283200</td>\n",
              "      <td>0.505572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.252600</td>\n",
              "      <td>0.502123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>0.113600</td>\n",
              "      <td>0.506024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.401700</td>\n",
              "      <td>0.510483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>0.427400</td>\n",
              "      <td>0.511284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.898200</td>\n",
              "      <td>0.507937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>0.279800</td>\n",
              "      <td>0.509182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.468300</td>\n",
              "      <td>0.508732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>0.314200</td>\n",
              "      <td>0.503896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.079300</td>\n",
              "      <td>0.504587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>0.378000</td>\n",
              "      <td>0.516470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.550400</td>\n",
              "      <td>0.510021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>0.297800</td>\n",
              "      <td>0.507391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.279700</td>\n",
              "      <td>0.512758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.678500</td>\n",
              "      <td>0.506754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.327800</td>\n",
              "      <td>0.511610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>0.574900</td>\n",
              "      <td>0.507341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.297400</td>\n",
              "      <td>0.512854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>0.157300</td>\n",
              "      <td>0.512152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.393700</td>\n",
              "      <td>0.518965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>0.188500</td>\n",
              "      <td>0.516238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>0.535100</td>\n",
              "      <td>0.514701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>0.670300</td>\n",
              "      <td>0.512537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>0.335400</td>\n",
              "      <td>0.513764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>0.398200</td>\n",
              "      <td>0.519484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>0.057600</td>\n",
              "      <td>0.510870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>0.024100</td>\n",
              "      <td>0.512903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>0.368800</td>\n",
              "      <td>0.513735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>0.263700</td>\n",
              "      <td>0.514950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.283200</td>\n",
              "      <td>0.517952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>0.351400</td>\n",
              "      <td>0.517566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.223200</td>\n",
              "      <td>0.516571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>0.345700</td>\n",
              "      <td>0.513987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>0.486600</td>\n",
              "      <td>0.517868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>0.797100</td>\n",
              "      <td>0.516308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>0.299300</td>\n",
              "      <td>0.514679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>0.376200</td>\n",
              "      <td>0.514150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>0.573400</td>\n",
              "      <td>0.515583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.840500</td>\n",
              "      <td>0.515852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.099400</td>\n",
              "      <td>0.515167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=64000, training_loss=0.7564709523189813, metrics={'train_runtime': 3058.9937, 'train_samples_per_second': 41.844, 'train_steps_per_second': 20.922, 'total_flos': 0.0, 'train_loss': 0.7564709523189813, 'epoch': 400.0})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_examples = 200\n",
        "data = generate_data(n_examples)\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZjq73ar0qWf",
        "outputId": "ff18f5b2-009a-462f-8342-9840db8f69eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['shape_list', 'code_str', 'text', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [d[\"text\"] for d in data]\n",
        "code_lists = [d[\"code_str\"] for d in data]"
      ],
      "metadata": {
        "id": "ia4ctxrX04LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = TranslationDataset(texts, code_lists, MyTokenizer(VOCABULARY), MyRandomMaskTokenizer(CODE_VOCABULARY), MAX_LEN_SENTENCE, tgt_max_len=MAX_CODE_LEN)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=2)\n",
        "total_correct = 0\n",
        "total_samples = len(texts)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in eval_loader:\n",
        "        src_tk = batch['source_tokens']\n",
        "        tgt_tk = batch['target_tokens']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        outputs = model(source_tokens=src_tk, target_tokens=tgt_tk, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=-1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHnXexw-WNG9",
        "outputId": "5cc9b3f6-48c0-4a99-e466-54ec2c8efc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 81.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text, src_tokenizer, tgt_tokenizer, src_max_length, tgt_max_length):\n",
        "    src_tokens = src_tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=src_max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "    translation = \"\"\n",
        "    token_count = 0\n",
        "    while True and token_count < tgt_max_length:\n",
        "      src_token_list = src_tokens[\"input_ids\"].unsqueeze(0)\n",
        "      tgt_tokens = tgt_tokenizer.encode_plus(\n",
        "            translation,\n",
        "            add_special_tokens=True,\n",
        "            max_length=tgt_max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "      tgt_token_list = tgt_tokens[\"input_ids\"].unsqueeze(0)\n",
        "      print(f\"src_token_list: {src_token_list}\")\n",
        "      print(f\"tgt_token_list: {tgt_token_list}\")\n",
        "      print(f\"token_list: {[CODE_VOCABULARY[tk] for tk in tgt_token_list[0]]}\")\n",
        "      att_mask = torch.cat((torch.ones(token_count), torch.zeros(tgt_token_list.shape[1] - token_count))).unsqueeze(0)\n",
        "      outputs = model(source_tokens=src_token_list, target_tokens=tgt_token_list, attention_mask=att_mask)\n",
        "      print(f\"outputs: {outputs}\")\n",
        "      _, pred = torch.max(outputs, dim=-1)\n",
        "      next_word = CODE_VOCABULARY[pred]\n",
        "      if next_word == EOS_TOKEN:\n",
        "        break\n",
        "      translation += \" \"  + next_word\n",
        "      token_count += 1\n",
        "    return translation\n",
        "\n",
        "n_examples = 10\n",
        "data = generate_data(n_examples)\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sN-d39Qs_xV",
        "outputId": "7813f139-8c7a-4ea2-9843-0896564730e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['shape_list', 'code_str', 'text', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for d in data:\n",
        "    print(d[\"text\"])\n",
        "    print(translate(d[\"text\"], MyTokenizer(VOCABULARY), MyRandomMaskTokenizer(CODE_VOCABULARY), MAX_LEN_SENTENCE, MAX_CODE_LEN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsgL4TBL5aWq",
        "outputId": "10d453aa-4bf7-4cad-ad2a-d38ac02d39c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a triangle\n",
            "src_token_list: tensor([[10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 4, 3, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "src_token_list: tensor([[10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 4, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.8530, -3.3643, -2.3866, -2.0576,  0.1915, -2.0823,  3.7314, -0.0846,\n",
            "         -0.6236, -0.6968, -2.0890,  0.0260]])\n",
            "src_token_list: tensor([[10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 4, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-0.1391, -0.3174,  0.0599, -0.2782,  2.1082,  0.3353,  2.4828, -2.2042,\n",
            "         -1.3575, -0.5411,  1.2055, -1.4796]])\n",
            "src_token_list: tensor([[10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 6, 4, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'EOS', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.9368, -2.8917, -2.4959, -1.9050, -0.6992, -2.5069, -1.4612, -1.2602,\n",
            "         -1.8323, -1.2071,  6.9075, -0.3139]])\n",
            "src_token_list: tensor([[10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 10,  4,  3,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 't', 'EOS', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.1910, -2.2857, -1.8141, -1.4238, -1.1943, -1.9431, -2.0617, -0.5907,\n",
            "         -1.0828, -0.3019,  6.1786, -1.2034]])\n",
            "src_token_list: tensor([[10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 10, 10,  4,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 't', 't', 'EOS', 'PAD']\n",
            "outputs: tensor([[-1.9665, -2.2726, -1.7841, -2.3099, -2.4002, -1.8632,  0.3655, -0.2495,\n",
            "          0.9604,  0.0778, -0.7965,  7.3388]])\n",
            "src_token_list: tensor([[10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 10, 10, 11,  4]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 't', 't', 'c', 'EOS']\n",
            "outputs: tensor([[-3.1293, -3.2671, -3.0675, -3.7718,  5.4600, -3.4517, -0.2754, -0.4834,\n",
            "         -0.6014, -2.8636, -1.7965, -0.8413]])\n",
            " CLS 1 1 t t c\n",
            "one circle and a circle\n",
            "src_token_list: tensor([[10,  5,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 4, 3, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "src_token_list: tensor([[10,  5,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 4, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.1252, -2.2251, -1.8480, -1.8597, -2.5083, -2.0988, -0.2854,  3.1801,\n",
            "          1.4690,  1.2559, -3.1229,  0.0072]])\n",
            "src_token_list: tensor([[10,  5,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 7, 4, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '2', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-1.8735, -2.2296, -1.5720, -1.5848, -2.6699, -1.6947,  0.9080,  2.8572,\n",
            "          1.4748,  1.4881, -3.7556,  0.0537]])\n",
            "src_token_list: tensor([[10,  5,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 7, 7, 4, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '2', '2', 'EOS', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-1.5808, -1.8741, -1.4244, -2.0224, -2.1142, -1.4474,  0.6336, -0.7820,\n",
            "          0.7171,  0.0278, -0.3418,  7.3446]])\n",
            "src_token_list: tensor([[10,  5,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  7,  7, 11,  4,  3,  3]])\n",
            "token_list: ['CLS', 'CLS', '2', '2', 'c', 'EOS', 'PAD', 'PAD']\n",
            "outputs: tensor([[-1.9371, -1.7641, -1.7655, -2.1229, -1.7177, -1.8214, -0.9535, -1.6101,\n",
            "         -0.4541, -0.8454,  3.6171,  7.0150]])\n",
            "src_token_list: tensor([[10,  5,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  7,  7, 11, 11,  4,  3]])\n",
            "token_list: ['CLS', 'CLS', '2', '2', 'c', 'c', 'EOS', 'PAD']\n",
            "outputs: tensor([[ 1.5248,  0.8872,  1.6005,  0.8272,  0.7873,  1.7407,  1.0425, -2.9031,\n",
            "         -0.5592,  0.8288,  3.2004, -0.9953]])\n",
            "src_token_list: tensor([[10,  5,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  7,  7, 11, 11, 10,  4]])\n",
            "token_list: ['CLS', 'CLS', '2', '2', 'c', 'c', 't', 'EOS']\n",
            "outputs: tensor([[-5.3274, -5.2337, -4.8761, -4.2730,  4.8930, -5.0784,  0.1457,  0.7244,\n",
            "         -1.9311, -3.4686,  0.0365, -5.2421]])\n",
            " CLS 2 2 c c t\n",
            "one triangle and one circle and circle after triangle\n",
            "src_token_list: tensor([[10,  5,  0,  6,  5,  1,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 4, 3, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "src_token_list: tensor([[10,  5,  0,  6,  5,  1,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 4, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.9998, -3.5160, -2.5192, -2.1410, -0.0865, -2.2320,  3.7025,  0.2853,\n",
            "         -0.4791, -0.5790, -2.4538, -0.0548]])\n",
            "src_token_list: tensor([[10,  5,  0,  6,  5,  1,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 4, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-0.2085, -0.5351,  0.0032, -0.3482,  1.6253,  0.2915,  2.7832, -2.0750,\n",
            "         -1.0818, -0.3348,  0.6269, -0.8338]])\n",
            "src_token_list: tensor([[10,  5,  0,  6,  5,  1,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 6, 4, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'EOS', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-4.0255, -3.8663, -3.4973, -2.6155, -0.8116, -3.5599, -1.6986, -0.3608,\n",
            "         -1.8277, -1.5494,  6.3356, -0.7296]])\n",
            "src_token_list: tensor([[10,  5,  0,  6,  5,  1,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 10,  4,  3,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 't', 'EOS', 'PAD', 'PAD']\n",
            "outputs: tensor([[-4.2192, -4.1092, -3.7463, -3.0870, -2.6053, -3.9076, -2.0953,  0.2950,\n",
            "         -0.8132, -1.0649,  4.9920,  3.0099]])\n",
            "src_token_list: tensor([[10,  5,  0,  6,  5,  1,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 10, 10,  4,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 't', 't', 'EOS', 'PAD']\n",
            "outputs: tensor([[-3.3749, -3.0569, -3.0829, -3.0403, -1.9886, -3.1617, -1.0818, -0.7489,\n",
            "         -0.6192, -1.3933,  3.2922,  6.9936]])\n",
            "src_token_list: tensor([[10,  5,  0,  6,  5,  1,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 10, 10, 11,  4]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 't', 't', 'c', 'EOS']\n",
            "outputs: tensor([[-6.0098, -6.0622, -5.3980, -4.5474, -1.6921, -5.4378,  1.3384,  2.6453,\n",
            "          0.1720, -1.4467, -3.1661,  2.9082]])\n",
            "src_token_list: tensor([[10,  5,  0,  6,  5,  1,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 10, 10, 11, 11]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 't', 't', 'c', 'c']\n",
            "outputs: tensor([[-4.2475, -4.0766, -4.0209, -4.1348,  5.9550, -4.2832,  0.2843, -0.0269,\n",
            "         -1.5172, -3.5803, -1.3958, -2.9239]])\n",
            " CLS 1 1 t t c c\n",
            "a triangle and one triangle and one circle\n",
            "src_token_list: tensor([[10,  4,  0,  6,  5,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 4, 3, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "src_token_list: tensor([[10,  4,  0,  6,  5,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 4, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-1.0396, -1.4994, -0.7405, -0.8802,  1.0120, -0.4277,  3.3160, -1.4491,\n",
            "         -0.8564, -0.3646, -0.4768, -0.3585]])\n",
            "src_token_list: tensor([[10,  4,  0,  6,  5,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 4, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[ 0.4619,  0.2896,  0.5925,  0.0532,  2.1283,  0.8320,  2.0866, -2.5269,\n",
            "         -1.2395, -0.3247,  1.6532, -1.1849]])\n",
            " CLS 1\n",
            "a circle\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 4, 3, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 4, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-3.7532, -4.0671, -3.1996, -2.5458,  0.6946, -2.8792,  3.7907,  0.3490,\n",
            "         -1.0933, -1.2871, -2.0535, -1.1271]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 4, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-1.1326, -1.1911, -0.8318, -0.8280,  2.3826, -0.5257,  2.7668, -1.6864,\n",
            "         -1.7145, -1.0548,  0.9031, -2.3666]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 6, 4, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'EOS', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.2884, -2.0838, -2.1309, -2.5662, -2.2441, -2.2625, -0.6032, -0.1455,\n",
            "          0.4510, -0.4708,  0.4534,  7.7136]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 11,  4,  3,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'c', 'EOS', 'PAD', 'PAD']\n",
            "outputs: tensor([[-0.0239,  0.0162, -0.0731, -1.1261, -2.5793, -0.3306, -1.6076, -0.4668,\n",
            "          1.2447,  0.9257,  0.8888,  7.1158]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 11, 11,  4,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'c', 'c', 'EOS', 'PAD']\n",
            "outputs: tensor([[ 1.1465,  0.8662,  1.2019,  0.4013,  1.7597,  1.4043,  1.8110, -2.8131,\n",
            "         -0.8659,  0.1091,  1.7493, -0.2902]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 11, 11,  6,  4]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'c', 'c', '1', 'EOS']\n",
            "outputs: tensor([[-4.5321, -4.4078, -4.2485, -4.1797,  5.8520, -4.4983,  0.2525,  0.1028,\n",
            "         -1.6749, -3.6052, -0.9486, -3.7555]])\n",
            " CLS 1 1 c c 1\n",
            "circle after circle and a triangle\n",
            "src_token_list: tensor([[10,  1,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 4, 3, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "src_token_list: tensor([[10,  1,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 4, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-4.5578, -4.9019, -3.9560, -3.1590, -0.6597, -3.7619,  3.2480,  1.8519,\n",
            "         -0.3068, -0.8841, -3.4621, -0.4263]])\n",
            "src_token_list: tensor([[10,  1,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 4, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-1.9504, -2.1921, -1.5600, -1.3549,  1.6388, -1.2214,  3.4188, -1.0549,\n",
            "         -1.4142, -1.0146, -0.3625, -1.7095]])\n",
            "src_token_list: tensor([[10,  1,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 6, 4, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'EOS', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-1.1547, -0.8554, -1.1143, -1.8073, -2.7698, -1.3994, -1.9033,  0.3973,\n",
            "          0.9867,  0.4613,  0.8258,  6.7219]])\n",
            "src_token_list: tensor([[10,  1,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 11,  4,  3,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'c', 'EOS', 'PAD', 'PAD']\n",
            "outputs: tensor([[-1.6632, -1.6074, -1.5635, -2.1943, -2.7087, -1.8156, -1.2432,  0.4259,\n",
            "          1.1763,  0.3260, -0.1625,  7.1020]])\n",
            "src_token_list: tensor([[10,  1,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 11, 11,  4,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'c', 'c', 'EOS', 'PAD']\n",
            "outputs: tensor([[-4.6880, -4.5948, -4.0236, -2.7618,  0.8658, -3.9521,  0.2364,  0.6301,\n",
            "         -2.1528, -1.8615,  3.5029, -4.9546]])\n",
            "src_token_list: tensor([[10,  1,  8,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 11, 11, 10,  4]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'c', 'c', 't', 'EOS']\n",
            "outputs: tensor([[-5.5563, -5.2757, -4.9956, -4.0329,  4.3472, -5.0316,  0.9487,  0.8884,\n",
            "         -2.3738, -3.4390,  0.2443, -5.7943]])\n",
            " CLS 1 1 c c t\n",
            "triangle after circle then a circle\n",
            "src_token_list: tensor([[10,  0,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 4, 3, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "src_token_list: tensor([[10,  0,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 4, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.4112, -2.4233, -2.1084, -2.0157, -2.2754, -2.3585, -0.3335,  3.2986,\n",
            "          1.2803,  1.0291, -3.0126, -0.4306]])\n",
            "src_token_list: tensor([[10,  0,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 7, 4, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '2', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.1512, -2.3660, -1.8253, -1.7116, -2.3814, -1.9582,  0.7452,  3.0983,\n",
            "          1.2492,  1.2673, -3.5973, -0.7371]])\n",
            "src_token_list: tensor([[10,  0,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 7, 7, 4, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '2', '2', 'EOS', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.6079, -2.7362, -2.3789, -2.7189, -2.2534, -2.4395,  0.3592, -0.1339,\n",
            "          0.6113, -0.4111, -0.5483,  7.4683]])\n",
            "src_token_list: tensor([[10,  0,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  7,  7, 11,  4,  3,  3]])\n",
            "token_list: ['CLS', 'CLS', '2', '2', 'c', 'EOS', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.7394, -2.5895, -2.4839, -2.5801, -2.1815, -2.5611, -1.1530, -1.1183,\n",
            "         -0.4752, -0.9875,  3.7623,  6.8739]])\n",
            "src_token_list: tensor([[10,  0,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  7,  7, 11, 11,  4,  3]])\n",
            "token_list: ['CLS', 'CLS', '2', '2', 'c', 'c', 'EOS', 'PAD']\n",
            "outputs: tensor([[-1.5022, -1.9550, -1.0827, -0.7042,  0.9333, -0.9390,  0.9003, -1.1577,\n",
            "         -1.4862, -0.3291,  3.6787, -4.5930]])\n",
            "src_token_list: tensor([[10,  0,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  7,  7, 11, 11, 10,  4]])\n",
            "token_list: ['CLS', 'CLS', '2', '2', 'c', 'c', 't', 'EOS']\n",
            "outputs: tensor([[-5.5812, -5.4654, -5.0374, -4.1435,  4.0632, -5.1779,  0.2419,  1.0291,\n",
            "         -2.0472, -3.2372,  0.5643, -5.7405]])\n",
            " CLS 2 2 c c t\n",
            "one circle and a triangle\n",
            "src_token_list: tensor([[10,  5,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 4, 3, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "src_token_list: tensor([[10,  5,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 4, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.9365, -3.2665, -2.4537, -2.0099,  0.9897, -2.1173,  3.7402, -0.2715,\n",
            "         -1.1641, -1.0980, -1.4559, -1.2113]])\n",
            "src_token_list: tensor([[10,  5,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 4, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-0.4608, -0.5164, -0.2329, -0.4437,  2.4490,  0.0436,  2.4028, -2.0767,\n",
            "         -1.6205, -0.8302,  1.4024, -2.1167]])\n",
            " CLS 1\n",
            "one triangle then one triangle and a circle\n",
            "src_token_list: tensor([[10,  5,  0,  7,  5,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 4, 3, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "src_token_list: tensor([[10,  5,  0,  7,  5,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 4, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.5250, -3.0991, -2.0918, -1.8829,  0.0177, -1.7976,  3.6689, -0.2370,\n",
            "         -0.4541, -0.4875, -2.0742,  0.4389]])\n",
            "src_token_list: tensor([[10,  5,  0,  7,  5,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 4, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[ 0.1111, -0.1564,  0.2859, -0.1544,  1.8394,  0.5558,  2.5034, -2.2936,\n",
            "         -1.1458, -0.3161,  1.0693, -0.9714]])\n",
            "src_token_list: tensor([[10,  5,  0,  7,  5,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 6, 4, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'EOS', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.7103, -2.5864, -2.3667, -2.0780, -1.8128, -2.4807, -2.1883, -1.0896,\n",
            "         -1.1548, -0.8850,  6.5095,  2.7336]])\n",
            "src_token_list: tensor([[10,  5,  0,  7,  5,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 10,  4,  3,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 't', 'EOS', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.5809, -2.4936, -2.3395, -2.4343, -3.2623, -2.6144, -2.2734,  0.5719,\n",
            "          0.5004,  0.0503,  2.6527,  5.4048]])\n",
            "src_token_list: tensor([[10,  5,  0,  7,  5,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 10, 11,  4,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 't', 'c', 'EOS', 'PAD']\n",
            "outputs: tensor([[-3.5039, -3.2650, -3.2305, -3.3357, -1.9296, -3.3241, -0.2171,  0.1198,\n",
            "          0.1142, -1.1641,  0.1579,  7.5423]])\n",
            "src_token_list: tensor([[10,  5,  0,  7,  5,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 10, 11, 11,  4]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 't', 'c', 'c', 'EOS']\n",
            "outputs: tensor([[-3.6320, -3.5651, -3.4800, -3.8467,  5.9690, -3.7712,  0.1342, -0.4008,\n",
            "         -1.2833, -3.3227, -1.3302, -2.3676]])\n",
            " CLS 1 1 t c c\n",
            "a circle\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 4, 3, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 4, 3, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-3.7532, -4.0671, -3.1996, -2.5458,  0.6946, -2.8792,  3.7907,  0.3490,\n",
            "         -1.0933, -1.2871, -2.0535, -1.1271]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 4, 3, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-1.1326, -1.1911, -0.8318, -0.8280,  2.3826, -0.5257,  2.7668, -1.6864,\n",
            "         -1.7145, -1.0548,  0.9031, -2.3666]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[0, 0, 6, 6, 4, 3, 3, 3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'EOS', 'PAD', 'PAD', 'PAD']\n",
            "outputs: tensor([[-2.2884, -2.0838, -2.1309, -2.5662, -2.2441, -2.2625, -0.6032, -0.1455,\n",
            "          0.4510, -0.4708,  0.4534,  7.7136]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 11,  4,  3,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'c', 'EOS', 'PAD', 'PAD']\n",
            "outputs: tensor([[-0.0239,  0.0162, -0.0731, -1.1261, -2.5793, -0.3306, -1.6076, -0.4668,\n",
            "          1.2447,  0.9257,  0.8888,  7.1158]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 11, 11,  4,  3]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'c', 'c', 'EOS', 'PAD']\n",
            "outputs: tensor([[ 1.1465,  0.8662,  1.2019,  0.4013,  1.7597,  1.4043,  1.8110, -2.8131,\n",
            "         -0.8659,  0.1091,  1.7493, -0.2902]])\n",
            "src_token_list: tensor([[10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tgt_token_list: tensor([[ 0,  0,  6,  6, 11, 11,  6,  4]])\n",
            "token_list: ['CLS', 'CLS', '1', '1', 'c', 'c', '1', 'EOS']\n",
            "outputs: tensor([[-4.5321, -4.4078, -4.2485, -4.1797,  5.8520, -4.4983,  0.2525,  0.1028,\n",
            "         -1.6749, -3.6052, -0.9486, -3.7555]])\n",
            " CLS 1 1 c c 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouHaPl3T5goI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}