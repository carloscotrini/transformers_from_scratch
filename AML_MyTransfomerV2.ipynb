{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqHWafU3SjSUbG6YEdOdbp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carloscotrini/transformers_from_scratch/blob/main/AML_MyTransfomerV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GWhXikQiMT2",
        "outputId": "9a251efe-3540-4d87-9bcf-99543e4449f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one triangle\n",
            "triangle after triangle\n",
            "two triangles\n",
            "circle after triangle then a circle and one triangle\n",
            "one triangle\n",
            "one circle\n",
            "circle after triangle and one triangle\n",
            "triangle after triangle and a triangle\n",
            "one circle\n",
            "one circle and circle after triangle and a circle\n",
            "circle after circle and circle after triangle\n",
            "one circle then triangle after triangle and one circle\n",
            "circle after triangle\n",
            "a circle and one triangle and one triangle then a circle\n",
            "one circle\n",
            "one circle\n",
            "triangle after circle\n",
            "circle after triangle\n",
            "triangle after circle\n",
            "triangle after circle and a triangle\n",
            "triangle after triangle\n",
            "circle after triangle\n",
            "triangle after circle then one triangle\n",
            "two triangles\n",
            "triangle after triangle\n",
            "triangle after triangle\n",
            "one circle and one circle\n",
            "a triangle and two triangles\n",
            "triangle after triangle\n",
            "circle after triangle then triangle after circle\n",
            "a triangle\n",
            "a triangle then a circle\n",
            "triangle after circle\n",
            "a circle and circle after circle and one triangle\n",
            "triangle after triangle\n",
            "one triangle\n",
            "a triangle\n",
            "one circle\n",
            "a triangle then a circle and one triangle\n",
            "a triangle\n",
            "circle after triangle then a triangle\n",
            "triangle after circle then a circle and one triangle\n",
            "circle after triangle then one triangle\n",
            "one triangle\n",
            "triangle after triangle and one circle\n",
            "triangle after triangle and one triangle and one triangle\n",
            "two triangles\n",
            "triangle after triangle\n",
            "triangle after triangle then two circles\n",
            "a circle\n",
            "two circles\n",
            "a triangle then one triangle\n",
            "one circle and one triangle then circle after triangle\n",
            "a triangle then one circle then circle after triangle\n",
            "a circle and two circles\n",
            "one circle then one triangle and one circle\n",
            "circle after triangle\n",
            "one triangle\n",
            "one triangle and one triangle\n",
            "a triangle and circle after circle and one circle\n",
            "a triangle\n",
            "circle after triangle then circle after circle\n",
            "triangle after circle then one circle then one circle\n",
            "a triangle\n",
            "triangle after circle\n",
            "two circles\n",
            "one circle\n",
            "one circle\n",
            "one circle and one triangle and triangle after circle\n",
            "a circle\n",
            "a circle\n",
            "one triangle then one triangle then circle after circle\n",
            "one triangle\n",
            "triangle after triangle\n",
            "a triangle and one triangle\n",
            "triangle after circle then a triangle\n",
            "one circle\n",
            "triangle after triangle and triangle after triangle\n",
            "circle after circle and one triangle\n",
            "circle after triangle and circle after circle\n",
            "circle after circle then circle after circle\n",
            "circle after circle then one triangle then one circle\n",
            "a triangle then triangle after triangle\n",
            "one triangle then circle after triangle then one triangle\n",
            "a circle then two circles\n",
            "one triangle and a circle\n",
            "a triangle then a circle then a triangle\n",
            "a triangle then circle after triangle and a circle\n",
            "one circle and triangle after triangle then a triangle\n",
            "two circles\n",
            "circle after triangle\n",
            "one triangle then a triangle\n",
            "circle after circle\n",
            "one triangle\n",
            "triangle after circle\n",
            "one triangle then two triangles\n",
            "circle after circle then one triangle\n",
            "a triangle\n",
            "a circle\n",
            "circle after circle\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "SHAPES = [\"triangle\", \"circle\"]\n",
        "PLURALS = [shape + \"s\" for shape in SHAPES]\n",
        "ARTICLES = [\"a\", \"one\"]\n",
        "TWO_ARTICLES = [\"two\"]\n",
        "CONNECTORS = [\"and\", \"then\"]\n",
        "REVERSE_CONNECTORS = [\"after\"]\n",
        "CLASS_TOKEN = \"CLS\"\n",
        "MASK_TOKEN = \"MASK\"\n",
        "SEP_TOKEN = \"SEP\"\n",
        "PAD_TOKEN = \"PAD\"\n",
        "EOS_TOKEN = \"EOS\"\n",
        "SPECIAL_TOKENS = [CLASS_TOKEN, MASK_TOKEN, SEP_TOKEN, PAD_TOKEN, EOS_TOKEN]\n",
        "VOCABULARY = SHAPES + PLURALS + ARTICLES + CONNECTORS + REVERSE_CONNECTORS + TWO_ARTICLES + SPECIAL_TOKENS\n",
        "MAX_LEN_SENTENCE = 16 # Maximum possible length of a sequence\n",
        "\n",
        "def generate_descr_from_list(r):\n",
        "  if len(r) > 4:\n",
        "    raise Exception(\"Too many items\")\n",
        "  elif len(r) == 0:\n",
        "    return \"\"\n",
        "  elif len(r) == 1:\n",
        "    article = random.choice(ARTICLES)\n",
        "    return \"{} {}\".format(article, r[0])\n",
        "  else:\n",
        "    reversed_descr = random.random() > 0.5\n",
        "    if reversed_descr:\n",
        "      descr = \"{} {} {}\".format(r[1], random.choice(REVERSE_CONNECTORS), r[0])\n",
        "      if len(r) > 2:\n",
        "        return descr + \" \" + random.choice(CONNECTORS) + \" \" + generate_descr_from_list(r[2:])\n",
        "      return descr\n",
        "    if r[0] == r[1]:\n",
        "      plural_desc = random.random() > 0.5\n",
        "      if plural_desc:\n",
        "          return \"{} {}s\".format(random.choice(TWO_ARTICLES), r[0])\n",
        "    return generate_descr_from_list([r[0]]) + \" \" + random.choice(CONNECTORS) + \" \" + generate_descr_from_list(r[1:])\n",
        "\n",
        "\n",
        "def generate_random_shapes():\n",
        "  num_shapes = random.randint(1, 4)\n",
        "  result = []\n",
        "  for _ in range(num_shapes):\n",
        "    result.append(random.choice(SHAPES))\n",
        "  return result\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "  print(generate_descr_from_list(generate_random_shapes()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import random\n",
        "\n",
        "def generate_image(word_list, filename):\n",
        "    # Create a blank 32x32 image\n",
        "    image_size = 32\n",
        "    patch_size = 16\n",
        "    image = Image.new(\"1\", (image_size, image_size), 1)  # '1' for 1-bit pixels, black and white\n",
        "\n",
        "    for i, word in enumerate(word_list):\n",
        "        if word not in [\"triangle\", \"circle\"]:\n",
        "            continue\n",
        "\n",
        "        # Determine the top-left corner of the patch\n",
        "        x_offset = (i % 2) * patch_size\n",
        "        y_offset = (i // 2) * patch_size\n",
        "\n",
        "        # Draw the shape in the corresponding patch\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        if word == \"triangle\":\n",
        "            points = [(random.randint(x_offset, x_offset + patch_size), random.randint(y_offset, y_offset + patch_size)) for _ in range(3)]\n",
        "            draw.polygon(points, fill=0)\n",
        "        elif word == \"circle\":\n",
        "            radius = random.randint(2, patch_size // 2)\n",
        "            center_x = random.randint(x_offset + radius, x_offset + patch_size - radius)\n",
        "            center_y = random.randint(y_offset + radius, y_offset + patch_size - radius)\n",
        "            draw.ellipse([center_x - radius, center_y - radius, center_x + radius, center_y + radius], fill=0)\n",
        "\n",
        "    # Save the image to the specified filename\n",
        "    image.save(filename)\n",
        "\n",
        "def plot_image(filename):\n",
        "    # Open the image\n",
        "    image = Image.open(filename)\n",
        "\n",
        "    # Convert the image to a NumPy array\n",
        "    image_array = np.array(image)\n",
        "\n",
        "    # Plot the image\n",
        "    plt.imshow(image_array)\n",
        "    plt.axis('off')  # Turn off axis labels\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "generate_image([\"circle\", \"triangle\", \"circle\"], \"output_image.png\")\n"
      ],
      "metadata": {
        "id": "wTQhOfTfnv11"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBER_CODES = [str(i) for i in range(5)]\n",
        "SHAPE_CODES = [shape[0] for shape in SHAPES]\n",
        "CODES = NUMBER_CODES + SHAPE_CODES\n",
        "CODE_VOCABULARY = SPECIAL_TOKENS + CODES\n",
        "\n",
        "def generate_code_str(shape_list):\n",
        "  codes = []\n",
        "  i = 0\n",
        "  while i < len(shape_list):\n",
        "    j = i + 1\n",
        "    while j < len(shape_list) and shape_list[i] == shape_list[j]:\n",
        "      j += 1\n",
        "    codes.append(f\"{NUMBER_CODES[j-i]} {shape_list[i][0]}\")\n",
        "    i = j\n",
        "  return \" \".join(codes)"
      ],
      "metadata": {
        "id": "fKuFwFcvmroH"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(n_samples):\n",
        "  examples = []\n",
        "  for i in range(n_samples):\n",
        "    shape_list = generate_random_shapes()\n",
        "    code_str = generate_code_str(shape_list)\n",
        "    text = generate_descr_from_list(shape_list)\n",
        "    filename = f\"f{i}.png\"\n",
        "    generate_image(shape_list, filename)\n",
        "    examples.append({\"shape_list\": shape_list, \"code_str\": code_str, \"text\": text, \"filename\": filename})\n",
        "  return examples\n"
      ],
      "metadata": {
        "id": "VvfchFGDpLHC"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "# Step 1: Prepare the Dataset\n",
        "class CountingFiguresDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "TRF1Xd44rZsx"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88p-uxGSSEbs",
        "outputId": "02c8f1e9-f135-434b-c328-cf3bbed5310a"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from collections import OrderedDict\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "class MyTokenizer:\n",
        "    def __init__(self, vocabulary):\n",
        "        self.vocabulary = vocabulary\n",
        "\n",
        "    def encode(self, text, add_special_tokens=True, max_length=MAX_LEN_SENTENCE, return_token_type_ids=False, padding='max_length', return_attention_mask=True, return_tensors='pt'):\n",
        "        tokens = text.split()\n",
        "        tokens.append(EOS_TOKEN)\n",
        "        input_ids = [self.vocabulary.index(token) for token in tokens]\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        if add_special_tokens:\n",
        "            input_ids = [self.vocabulary.index(CLASS_TOKEN)] + input_ids\n",
        "            attention_mask += [1]\n",
        "\n",
        "        sen_len = len(input_ids)\n",
        "        if len(input_ids) > max_length:\n",
        "            input_ids = input_ids[:max_length]\n",
        "            attention_mask = attention_mask[:max_length]\n",
        "            sen_len = max_length\n",
        "        else:\n",
        "            pad_length = max_length - len(input_ids)\n",
        "            if pad_length >= 0:\n",
        "                input_ids += [self.vocabulary.index(PAD_TOKEN)] * pad_length\n",
        "                attention_mask += [0] * pad_length\n",
        "\n",
        "        return sen_len, input_ids, attention_mask\n",
        "\n",
        "    def encode_plus(self, text, add_special_tokens=True, max_length=MAX_LEN_SENTENCE, return_token_type_ids=False, padding='max_length', return_attention_mask=True, return_tensors='pt'):\n",
        "        _, input_ids, attention_mask = self.encode(text, add_special_tokens, max_length, return_token_type_ids, padding, return_attention_mask, return_tensors)\n",
        "        if return_attention_mask:\n",
        "          return {\n",
        "              'input_ids': torch.tensor(input_ids),\n",
        "              'attention_mask': torch.tensor(attention_mask)\n",
        "          }\n",
        "        else:\n",
        "          return {\n",
        "              'input_ids': torch.tensor(input_ids)\n",
        "          }\n"
      ],
      "metadata": {
        "id": "S1ey-4OGDwWF"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_key_dim, hidden_val_dim, enc_emb_dim=None, num_heads=1):\n",
        "        \"\"\"\n",
        "          Implements an attention mechanism\n",
        "\n",
        "          Args:\n",
        "          input_dim: Dimensionality of input embedding.\n",
        "          hidden_key_dim: Dimensionality of key and query vectors.\n",
        "          hidden_val_dim: Dimensionality of value vectors.\n",
        "          enc_emb_dim: Dimensionality of encoder embeddings. If None, self-attention is used.\n",
        "          mask: Whether to apply masking. If True, the attention scores for masked positions are set to -inf.\n",
        "          num_heads: Number of attention heads.\n",
        "        \"\"\"\n",
        "        super(MyAttention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.enc_emb_dim = enc_emb_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.to_q = nn.Linear(self.input_dim, self.hidden_key_dim * self.num_heads, bias=False)\n",
        "\n",
        "        for i in range(self.num_heads):\n",
        "          if enc_emb_dim is None:\n",
        "              self.to_k = nn.Linear(self.input_dim, self.hidden_key_dim * self.num_heads, bias=False)\n",
        "              self.to_v = nn.Linear(self.input_dim, self.hidden_val_dim * self.num_heads, bias=False)\n",
        "          else:\n",
        "              self.to_k = nn.Linear(self.enc_emb_dim, self.hidden_key_dim * self.num_heads, bias=False)\n",
        "              self.to_v = nn.Linear(self.enc_emb_dim, self.hidden_val_dim * self.num_heads, bias=False)\n",
        "\n",
        "        self.to_out = nn.Linear(self.hidden_val_dim * self.num_heads, self.input_dim)\n",
        "\n",
        "    def forward(self, embeddings, encoder_embeddings=None, attention_mask=None):\n",
        "\n",
        "        if encoder_embeddings is not None and attention_mask is not None:\n",
        "            raise Exception(\"In cross attention there is no masking.\")\n",
        "\n",
        "        if encoder_embeddings is None:\n",
        "            Q = self.to_q(embeddings)\n",
        "            K = self.to_k(embeddings)\n",
        "            V = self.to_v(embeddings)\n",
        "        else:\n",
        "            Q = self.to_q(embeddings)\n",
        "            K = self.to_k(encoder_embeddings)\n",
        "            V = self.to_v(encoder_embeddings)\n",
        "\n",
        "        Q = rearrange(Q, 'B T (H D) -> B H T D', H=self.num_heads, D=self.hidden_key_dim)\n",
        "        K = rearrange(K, 'B T (H D) -> B H T D', H=self.num_heads, D=self.hidden_key_dim)\n",
        "        V = rearrange(V, 'B T (H D) -> B H T D', H=self.num_heads, D=self.hidden_val_dim)\n",
        "\n",
        "        scores = torch.einsum(\"BHTD,BHSD->BHTS\", Q, K)\n",
        "\n",
        "        if attention_mask is not None and encoder_embeddings is None:\n",
        "            # Originally, attention_mask has shape (batch_size, sequence_len)\n",
        "            # To ensure propagation to the scores matrix, which has shape (batch_size, num_heads, sequence_len, sequence_len),\n",
        "            # We need to make attention_mask's shape (batch_size, 1, 1, sequence_len)\n",
        "            # We do this with the unsqueeze method, which adds a new dimension.\n",
        "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
        "            print(scores.shape)\n",
        "            print(attention_mask.shape)\n",
        "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
        "\n",
        "        attnmats = F.softmax(scores / math.sqrt(self.hidden_key_dim), dim=-1)\n",
        "\n",
        "        ctx_vecs = torch.einsum(\"BHTS,BHSD->BHTD\", attnmats, V)\n",
        "        ctx_vecs = rearrange(ctx_vecs, 'B H T D -> B T (H D)', H=self.num_heads, D=self.hidden_val_dim)\n",
        "        return self.to_out(ctx_vecs)"
      ],
      "metadata": {
        "id": "aFYriqViEvVt"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=MAX_LEN_SENTENCE):\n",
        "        super(MyPositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a matrix of shape (max_len, d_model) with all zeros\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Create a column vector of shape (max_len, 1) with values [0, 1, ..., max_len-1]\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Create a row vector of shape (1, d_model // 2) with values [0, 1, ..., d_model//2-1]\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even indices and cosine to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add a batch dimension (1, max_len, d_model) and register as buffer\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input tensor (B, T, D)\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "HJBjsyuMMeLO"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_key_dim, hidden_val_dim, output_dim, num_heads=1):\n",
        "        super(MyTransformerEncoderLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.attention = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=None, num_heads=self.num_heads)\n",
        "        self.norm1 = nn.LayerNorm(self.input_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, self.output_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(self.output_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        x = self.norm1(self.attention(x, attention_mask=attention_mask) + x)\n",
        "        x = self.norm2(self.feed_forward(x) + x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jt6_PYhSNLBh"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_tokens, input_dim, hidden_key_dim, hidden_val_dim, output_dim, max_length, num_layers=1, num_heads=1):\n",
        "        super(MyTransformerEncoder, self).__init__()\n",
        "        self.num_tokens = num_tokens\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.max_length = max_length\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.embedding = nn.Embedding(num_tokens, self.input_dim)\n",
        "        self.positional_encoding = MyPositionalEncoding(self.input_dim, max_length)\n",
        "        self.layers = nn.ModuleList([MyTransformerEncoderLayer(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, self.input_dim, num_heads) for _ in range(num_layers)])\n",
        "        self.linear = nn.Linear(self.input_dim, self.output_dim)\n",
        "        self.norm = nn.LayerNorm(self.output_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask=attention_mask)\n",
        "        x = self.linear(x)\n",
        "        x = self.norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "N90NAhGjMgZu"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerClassifier(nn.Module):\n",
        "    def __init__(self, transf_enc, num_classes):\n",
        "        super(MyTransformerClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.transf_enc = transf_enc\n",
        "\n",
        "        self.linear = nn.Linear(self.transf_enc.output_dim, self.num_classes)\n",
        "\n",
        "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
        "        x = self.transf_enc(input_ids, attention_mask)[:, 0, :] # Just the embedding of the first token, which is the CLS token.\n",
        "        logits = self.linear(x)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss = criterion(logits, labels)\n",
        "        return (loss, logits) if loss is not None else logits"
      ],
      "metadata": {
        "id": "thRPUXH4PtXH"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "\n",
        "# Step 2: Tokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer = MyTokenizer(VOCABULARY)\n",
        "tokenizer.encode_plus(\n",
        "            \"one circle after one circle and two triangles\",\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN_SENTENCE,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUw3D9j26rgn",
        "outputId": "935c4053-0da6-4347-a952-bb786dd03611"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([10,  5,  1,  8,  5,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset\n",
        "lengths = list(map(len, shape_lists))\n",
        "dataset = CountingFiguresDataset(descriptions, lengths, tokenizer, max_length=MAX_LEN_SENTENCE)"
      ],
      "metadata": {
        "id": "kgwzsdVxJIdL"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(VOCABULARY)\n",
        "input_dim = 16\n",
        "hidden_key_dim = 8\n",
        "hidden_val_dim = 8\n",
        "num_heads = 2\n",
        "output_dim = 16\n",
        "num_layers = 3\n",
        "num_labels = 5\n",
        "\n",
        "# Step 3: Model\n",
        "transf_enc = MyTransformerEncoder(num_tokens, input_dim=input_dim, hidden_key_dim=hidden_key_dim, hidden_val_dim=hidden_val_dim, output_dim=output_dim, max_length=MAX_LEN_SENTENCE, num_layers=num_layers, num_heads=num_heads)\n",
        "model = MyTransformerClassifier(transf_enc, num_labels)\n",
        "\n",
        "# Step 4: Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=200,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.001,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Split dataset into train and eval\n",
        "train_size = int(0.8 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
        "\n",
        "# Custom Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DQbq5O20F1Nx",
        "outputId": "241c1b37-fc5b-4ba3-a4e0-ece467580e35"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8000' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8000/8000 02:26, Epoch 200/200]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.361900</td>\n",
              "      <td>1.538641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.694700</td>\n",
              "      <td>1.529770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.382700</td>\n",
              "      <td>1.519997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.373100</td>\n",
              "      <td>1.509507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.534600</td>\n",
              "      <td>1.500978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.395300</td>\n",
              "      <td>1.490364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.456700</td>\n",
              "      <td>1.482291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.445700</td>\n",
              "      <td>1.473454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.439100</td>\n",
              "      <td>1.463923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.585500</td>\n",
              "      <td>1.457543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.476500</td>\n",
              "      <td>1.447391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.247200</td>\n",
              "      <td>1.435416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.461300</td>\n",
              "      <td>1.428436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.314300</td>\n",
              "      <td>1.419067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.433200</td>\n",
              "      <td>1.411634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.432900</td>\n",
              "      <td>1.405093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.368900</td>\n",
              "      <td>1.397938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.399400</td>\n",
              "      <td>1.390981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.414400</td>\n",
              "      <td>1.384047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.314200</td>\n",
              "      <td>1.377050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.325600</td>\n",
              "      <td>1.369573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.381200</td>\n",
              "      <td>1.361715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.419200</td>\n",
              "      <td>1.356788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.388100</td>\n",
              "      <td>1.349750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.428000</td>\n",
              "      <td>1.343521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.293500</td>\n",
              "      <td>1.333964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.290100</td>\n",
              "      <td>1.326188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.343000</td>\n",
              "      <td>1.317043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.328100</td>\n",
              "      <td>1.309026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.285300</td>\n",
              "      <td>1.302081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.252900</td>\n",
              "      <td>1.293967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.295600</td>\n",
              "      <td>1.284780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.400100</td>\n",
              "      <td>1.272470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.240900</td>\n",
              "      <td>1.264178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.240100</td>\n",
              "      <td>1.252161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.299900</td>\n",
              "      <td>1.237165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.254600</td>\n",
              "      <td>1.224718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.234900</td>\n",
              "      <td>1.212177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.170100</td>\n",
              "      <td>1.194784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.296900</td>\n",
              "      <td>1.178011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.149700</td>\n",
              "      <td>1.164436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.205900</td>\n",
              "      <td>1.148106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.087100</td>\n",
              "      <td>1.129657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.141500</td>\n",
              "      <td>1.110926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.176100</td>\n",
              "      <td>1.092943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.066800</td>\n",
              "      <td>1.078155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.148000</td>\n",
              "      <td>1.059933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.974400</td>\n",
              "      <td>1.043096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.033500</td>\n",
              "      <td>1.027284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.956900</td>\n",
              "      <td>1.008826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.052400</td>\n",
              "      <td>0.992122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.083100</td>\n",
              "      <td>0.976337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.997600</td>\n",
              "      <td>0.960860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.013500</td>\n",
              "      <td>0.945104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.926800</td>\n",
              "      <td>0.929183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.885000</td>\n",
              "      <td>0.914700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.946800</td>\n",
              "      <td>0.898965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.003900</td>\n",
              "      <td>0.881573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.911600</td>\n",
              "      <td>0.869019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.957400</td>\n",
              "      <td>0.853987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.823100</td>\n",
              "      <td>0.844691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.965500</td>\n",
              "      <td>0.830485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.713600</td>\n",
              "      <td>0.820232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.742300</td>\n",
              "      <td>0.803730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.812700</td>\n",
              "      <td>0.791181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.817800</td>\n",
              "      <td>0.780171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.785100</td>\n",
              "      <td>0.769105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.780200</td>\n",
              "      <td>0.758018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.884300</td>\n",
              "      <td>0.748977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.788400</td>\n",
              "      <td>0.738188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.668100</td>\n",
              "      <td>0.727934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.746400</td>\n",
              "      <td>0.715356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.633900</td>\n",
              "      <td>0.707738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.626900</td>\n",
              "      <td>0.698308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.720800</td>\n",
              "      <td>0.689299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.695000</td>\n",
              "      <td>0.685918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.500800</td>\n",
              "      <td>0.676901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.612000</td>\n",
              "      <td>0.665515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.746100</td>\n",
              "      <td>0.659148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0.655931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.749000</td>\n",
              "      <td>0.648096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.583900</td>\n",
              "      <td>0.639558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.636800</td>\n",
              "      <td>0.631945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.517000</td>\n",
              "      <td>0.627535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.582100</td>\n",
              "      <td>0.620318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.658100</td>\n",
              "      <td>0.615335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.583700</td>\n",
              "      <td>0.609070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.561600</td>\n",
              "      <td>0.603540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.610700</td>\n",
              "      <td>0.596668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.482800</td>\n",
              "      <td>0.593401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.464800</td>\n",
              "      <td>0.585614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.511700</td>\n",
              "      <td>0.580515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.598300</td>\n",
              "      <td>0.573359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.408900</td>\n",
              "      <td>0.567609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.450200</td>\n",
              "      <td>0.566094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.617000</td>\n",
              "      <td>0.560122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.365200</td>\n",
              "      <td>0.560125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.425800</td>\n",
              "      <td>0.552413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.650700</td>\n",
              "      <td>0.548586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.506500</td>\n",
              "      <td>0.544811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.441000</td>\n",
              "      <td>0.540499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.504100</td>\n",
              "      <td>0.535603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.585300</td>\n",
              "      <td>0.530975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.515200</td>\n",
              "      <td>0.528739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.657400</td>\n",
              "      <td>0.526537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.420900</td>\n",
              "      <td>0.521714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.582100</td>\n",
              "      <td>0.518778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.347400</td>\n",
              "      <td>0.519303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.388600</td>\n",
              "      <td>0.516546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.378800</td>\n",
              "      <td>0.515794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.398100</td>\n",
              "      <td>0.511142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.526100</td>\n",
              "      <td>0.506771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.479400</td>\n",
              "      <td>0.506572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.477700</td>\n",
              "      <td>0.507034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.380300</td>\n",
              "      <td>0.505254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.431000</td>\n",
              "      <td>0.503594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.345500</td>\n",
              "      <td>0.498920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.556200</td>\n",
              "      <td>0.495613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.503400</td>\n",
              "      <td>0.491769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.529500</td>\n",
              "      <td>0.490116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.369400</td>\n",
              "      <td>0.490142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.335700</td>\n",
              "      <td>0.486222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.475700</td>\n",
              "      <td>0.483139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.575800</td>\n",
              "      <td>0.483312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.465500</td>\n",
              "      <td>0.482886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.431100</td>\n",
              "      <td>0.479149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.280900</td>\n",
              "      <td>0.479637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.393800</td>\n",
              "      <td>0.479558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.460300</td>\n",
              "      <td>0.475406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.498300</td>\n",
              "      <td>0.474213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.486700</td>\n",
              "      <td>0.476001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.604600</td>\n",
              "      <td>0.471995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.578800</td>\n",
              "      <td>0.469865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.367200</td>\n",
              "      <td>0.467428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.351800</td>\n",
              "      <td>0.465054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.284300</td>\n",
              "      <td>0.464595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.683700</td>\n",
              "      <td>0.463978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.200800</td>\n",
              "      <td>0.460548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.341000</td>\n",
              "      <td>0.458876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.370700</td>\n",
              "      <td>0.458832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.265600</td>\n",
              "      <td>0.459607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.618900</td>\n",
              "      <td>0.457493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.495500</td>\n",
              "      <td>0.457132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.278400</td>\n",
              "      <td>0.454100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.256700</td>\n",
              "      <td>0.456251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.485800</td>\n",
              "      <td>0.452381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.537900</td>\n",
              "      <td>0.450419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.264600</td>\n",
              "      <td>0.451133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.243900</td>\n",
              "      <td>0.449727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.611900</td>\n",
              "      <td>0.448905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.350800</td>\n",
              "      <td>0.448007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.359600</td>\n",
              "      <td>0.447605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.419100</td>\n",
              "      <td>0.446556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.476400</td>\n",
              "      <td>0.444686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.442100</td>\n",
              "      <td>0.446289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.495500</td>\n",
              "      <td>0.443075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.443000</td>\n",
              "      <td>0.443144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.326600</td>\n",
              "      <td>0.444050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.431400</td>\n",
              "      <td>0.442897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.286100</td>\n",
              "      <td>0.441492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.383900</td>\n",
              "      <td>0.441824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.485500</td>\n",
              "      <td>0.440696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.431300</td>\n",
              "      <td>0.439465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.459100</td>\n",
              "      <td>0.438579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.365900</td>\n",
              "      <td>0.439413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.290500</td>\n",
              "      <td>0.438660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.547900</td>\n",
              "      <td>0.438538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.292000</td>\n",
              "      <td>0.438027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.312800</td>\n",
              "      <td>0.436573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.277200</td>\n",
              "      <td>0.435936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.380000</td>\n",
              "      <td>0.435438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.458000</td>\n",
              "      <td>0.435634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.495400</td>\n",
              "      <td>0.434474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.391100</td>\n",
              "      <td>0.435071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.339700</td>\n",
              "      <td>0.434539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.406700</td>\n",
              "      <td>0.433762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.631300</td>\n",
              "      <td>0.433139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.193300</td>\n",
              "      <td>0.432842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.236900</td>\n",
              "      <td>0.432603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.341300</td>\n",
              "      <td>0.433072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.455200</td>\n",
              "      <td>0.432527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.327200</td>\n",
              "      <td>0.431464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.437600</td>\n",
              "      <td>0.430943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.422300</td>\n",
              "      <td>0.431052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.348200</td>\n",
              "      <td>0.431271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.346100</td>\n",
              "      <td>0.431024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.411800</td>\n",
              "      <td>0.431195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.446100</td>\n",
              "      <td>0.430851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.341900</td>\n",
              "      <td>0.430805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.413000</td>\n",
              "      <td>0.430406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.454800</td>\n",
              "      <td>0.430204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.223600</td>\n",
              "      <td>0.430006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.332500</td>\n",
              "      <td>0.429971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.356300</td>\n",
              "      <td>0.430308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.377400</td>\n",
              "      <td>0.430061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.280900</td>\n",
              "      <td>0.430173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.583600</td>\n",
              "      <td>0.429947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.384900</td>\n",
              "      <td>0.429987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.364800</td>\n",
              "      <td>0.430038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.331300</td>\n",
              "      <td>0.430006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8000, training_loss=0.7032698403298855, metrics={'train_runtime': 146.4317, 'train_samples_per_second': 109.266, 'train_steps_per_second': 54.633, 'total_flos': 0.0, 'train_loss': 0.7032698403298855, 'epoch': 200.0})"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Step 5: Evaluation\n",
        "def evaluate_model(texts, labels):\n",
        "    eval_dataset = CountingFiguresDataset(texts, labels, tokenizer, max_length=MAX_LEN_SENTENCE)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=2)\n",
        "    total_correct = 0\n",
        "    total_samples = len(labels)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=-1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "# Generate descriptions and images for each shape list\n",
        "\n",
        "test_shape_lists = [generate_random_shapes() for _ in range(100)]\n",
        "\n",
        "eval_descriptions = []\n",
        "eval_lengths = []\n",
        "for i, shape_list in enumerate(test_shape_lists):\n",
        "  eval_descriptions.append(generate_descr_from_list(shape_list))\n",
        "  eval_lengths.append(len(shape_list))\n",
        "\n",
        "evaluate_model(eval_descriptions, eval_lengths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4j1RRLb7hzX",
        "outputId": "11daf427-25b1-4bb4-f28d-398c00aaca55"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 88.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_key_dim, hidden_val_dim, output_dim, enc_emb_dim, num_heads):\n",
        "        super(MyTransformerDecoderLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.enc_emb_dim = enc_emb_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.masked_att = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=None, num_heads=self.num_heads)\n",
        "        self.norm1 = nn.LayerNorm(self.input_dim)\n",
        "        self.cross_att = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=self.enc_emb_dim, num_heads=self.num_heads)\n",
        "        self.norm2 = nn.LayerNorm(self.input_dim)\n",
        "        self.self_att = MyAttention(self.input_dim, self.hidden_key_dim, self.hidden_val_dim, enc_emb_dim=None, num_heads=self.num_heads)\n",
        "        self.norm3 = nn.LayerNorm(self.input_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, self.output_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, enc_emb, attention_mask):\n",
        "        x = self.norm1(x + self.masked_att(x, attention_mask=attention_mask))\n",
        "        x = self.norm2(x + self.cross_att(x, encoder_embeddings=enc_emb))\n",
        "        x = self.norm3(x + self.self_att(x, attention_mask=attention_mask))\n",
        "        x = self.feed_forward(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jeRheKQGyfmO"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_tokens, input_dim, hidden_key_dim, hidden_val_dim, output_dim, enc_emb_dim, max_length, num_layers, num_heads):\n",
        "        super(MyTransformerDecoder, self).__init__()\n",
        "        self.num_tokens = num_tokens\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_key_dim = hidden_key_dim\n",
        "        self.hidden_val_dim = hidden_val_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.enc_emb_dim = enc_emb_dim\n",
        "        self.max_length = max_length\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.embedding = nn.Embedding(num_tokens, self.input_dim)\n",
        "        self.positional_encoding = MyPositionalEncoding(self.input_dim, max_length)\n",
        "        self.layers = nn.ModuleList([MyTransformerDecoderLayer(input_dim=self.input_dim,\n",
        "                                                               hidden_key_dim=self.hidden_key_dim,\n",
        "                                                               hidden_val_dim=self.hidden_val_dim,\n",
        "                                                               enc_emb_dim=self.enc_emb_dim,\n",
        "                                                               output_dim=self.output_dim,\n",
        "                                                               num_heads=num_heads) for _ in range(num_layers)])\n",
        "        self.linear = nn.Linear(self.input_dim, self.output_dim)\n",
        "        self.norm = nn.LayerNorm(self.output_dim)\n",
        "\n",
        "    def forward(self, x, enc_emb, attention_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_emb, attention_mask=attention_mask)\n",
        "        x = self.linear(x)\n",
        "        x = self.norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ztvWkEGivzxx"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerTranslator(nn.Module):\n",
        "    def __init__(self, transf_enc, transf_dec, num_tokens_target_vocab):\n",
        "        super(MyTransformerTranslator, self).__init__()\n",
        "        self.transf_enc = transf_enc\n",
        "        self.transf_dec = transf_dec\n",
        "        self.linear = nn.Linear(self.transf_dec.output_dim, num_tokens_target_vocab)\n",
        "\n",
        "    def forward(self, source_tokens, target_tokens, attention_mask, labels=None):\n",
        "        source_embeddings = self.transf_enc(source_tokens)\n",
        "        decoded_embeddings = self.transf_dec(target_tokens, source_embeddings, attention_mask)[:, 0, :]\n",
        "        logits = self.linear(decoded_embeddings)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss = criterion(logits, labels)\n",
        "        return (loss, logits) if loss is not None else logits"
      ],
      "metadata": {
        "id": "LLudUsAtwhAs"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MyRandomMaskTokenizer(MyTokenizer):\n",
        "  def __init__(self, vocabulary):\n",
        "    super().__init__(vocabulary)\n",
        "\n",
        "  def encode_plus(self, text, add_special_tokens=True, max_length=MAX_LEN_SENTENCE, return_token_type_ids=False, padding='max_length', return_attention_mask=True, return_tensors='pt'):\n",
        "    sen_len, input_ids, attention_mask = super().encode(text, add_special_tokens, max_length, return_token_type_ids, padding, return_attention_mask, return_tensors)\n",
        "    if return_attention_mask:\n",
        "      new_len = random.randint(1, sen_len-1)\n",
        "      new_attention_mask = torch.cat((torch.ones(new_len), torch.zeros(len(input_ids) - new_len)))\n",
        "      return {\n",
        "              'input_ids': torch.tensor(input_ids),\n",
        "              'attention_mask': new_attention_mask,\n",
        "              'sen_len': new_len\n",
        "      }\n",
        "    else:\n",
        "      return {\n",
        "              'input_ids': torch.tensor(input_ids),\n",
        "              'sen_len': new_len\n",
        "      }\n",
        "\n",
        "tokenizer = MyRandomMaskTokenizer(CODE_VOCABULARY)\n",
        "tokenizer.encode_plus(\n",
        "            \"c 2 t 2\",\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN_SENTENCE,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvU4FiMZzYUU",
        "outputId": "ee2b19e1-559d-4e64-996d-7dd2eecf419a"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 0, 11,  7, 10,  7,  4,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3]),\n",
              " 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'sen_len': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_texts, tgt_texts, src_tokenizer, tgt_tokenizer, src_max_len, tgt_max_len):\n",
        "        self.src_texts = src_texts\n",
        "        self.tgt_texts = tgt_texts\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.src_max_len = src_max_len\n",
        "        self.tgt_max_len = tgt_max_len\n",
        "\n",
        "        self.tgt_encodings = [self.tgt_tokenizer.encode_plus(\n",
        "              txt,\n",
        "              add_special_tokens=True,\n",
        "              max_length=self.tgt_max_len,\n",
        "              return_token_type_ids=False,\n",
        "              padding='max_length',\n",
        "              return_attention_mask=True,\n",
        "              return_tensors='pt'\n",
        "            ) for txt in self.tgt_texts]\n",
        "\n",
        "        self.labels=[]\n",
        "        for tgt_encoding in self.tgt_encodings:\n",
        "          input_ids = tgt_encoding['input_ids']\n",
        "          label = input_ids[tgt_encoding['sen_len']]\n",
        "          self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_encoding = self.src_tokenizer.encode_plus(\n",
        "            self.src_texts[idx],\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.src_max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'source_embeddings': src_encoding['input_ids'].flatten(),\n",
        "            'target_embeddings': self.tgt_encodings[idx]['input_ids'].flatten(),\n",
        "            'attention_mask': self.tgt_encodings[idx]['attention_mask'].flatten(),\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "zgAr-duaEWP1"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CODE_LEN=8\n",
        "\n",
        "n_examples = 200\n",
        "data = generate_data(n_examples)\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPNYvGpfUVI9",
        "outputId": "e5e6f065-8967-45ab-fe78-c1e15470fa22"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['shape_list', 'code_str', 'text', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "descriptions = [d[\"text\"] for d in data]\n",
        "code_lists = [d[\"code_str\"] for d in data]"
      ],
      "metadata": {
        "id": "ujIdHu501lA2"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transl_dataset = TranslationDataset(descriptions, code_lists, MyTokenizer(VOCABULARY), MyRandomMaskTokenizer(CODE_VOCABULARY), MAX_LEN_SENTENCE, tgt_max_len=MAX_CODE_LEN)\n",
        "for example in transl_dataset:\n",
        "  print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4OF45l51jYX",
        "outputId": "869fce92-3107-451b-b6bc-ac74467e6b34"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  5,  1,  7,  5,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1,  6,  4,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  5,  1,  7,  4,  0,  7,  5,  1,  6,  5,  0, 14, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1,  7,  4,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  7,  5,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  7,  4,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  5,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  6,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  6,  5,  1,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  7, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  7,  4,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  5,  1,  6,  4,  0,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  4,  1,  7,  5,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  7,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(9)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1,  7,  5,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  4,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  4,  1,  7,  5,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  6,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  1,  7,  4,  0,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  1,  6,  1,  8,  0,  6,  4,  0, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  7,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  6,  4,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  6,  5,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  7,  1,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  5,  1,  7,  5,  1,  6,  4,  1,  7,  4,  1, 14, 13, 13, 13]), 'target_embeddings': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  7,  5,  1,  7,  4,  0,  7,  4,  1, 14, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  7,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  7,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  1,  6,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  7,  4,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  7, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  4,  1,  7,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  4,  1,  6,  0,  8,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  7,  0,  8,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  7,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  7,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  7,  5,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  4,  1,  7,  0,  8,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  8, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1,  7,  4,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  6, 11,  6]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1,  6,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  9, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  4,  1,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  5,  1,  7,  0,  8,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  6,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  4,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  5,  1,  6,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  7, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  7,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  6,  5,  0,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  6,  4,  1,  6,  9,  3, 14, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  7,  0,  8,  1,  6,  5,  1, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  6, 10,  6]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  6,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  5,  1,  6,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  6,  0,  8,  0,  6,  4,  0, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  9, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  9,  2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  7,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  8, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  6,  0,  8,  1,  7,  5,  0, 14, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  7, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(8)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  7,  5,  0,  7,  5,  1,  7,  4,  0, 14, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  5,  1,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  4,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 1., 0., 0., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  6,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 1., 1., 1., 0., 0., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  4,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  7,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(7)}\n",
            "{'source_embeddings': tensor([10,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  6,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  7, 10,  6, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  0,  7,  5,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  8, 10,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  7,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  7, 11,  6, 10,  4]), 'attention_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  6, 11,  4]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0.]), 'labels': tensor(4)}\n",
            "{'source_embeddings': tensor([10,  5,  0,  7,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  4,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  4,  3,  3,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  6, 11,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(10)}\n",
            "{'source_embeddings': tensor([10,  1,  8,  0,  7,  5,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 10,  7, 11,  4,  3,  3]), 'attention_mask': tensor([1., 0., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(6)}\n",
            "{'source_embeddings': tensor([10,  0,  8,  1, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]), 'target_embeddings': tensor([ 0,  6, 11,  6, 10,  4,  3,  3]), 'attention_mask': tensor([1., 1., 0., 0., 0., 0., 0., 0.]), 'labels': tensor(11)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_src_tokens=len(VOCABULARY)\n",
        "num_tgt_tokens=len(CODE_VOCABULARY)\n",
        "input_dim = 16\n",
        "hidden_key_dim = 8\n",
        "hidden_val_dim = 8\n",
        "num_heads = 2\n",
        "output_dim = 16\n",
        "num_layers = 3\n",
        "\n",
        "transf_enc = MyTransformerEncoder(num_src_tokens, input_dim=input_dim, hidden_key_dim=hidden_key_dim, hidden_val_dim=hidden_val_dim, output_dim=output_dim, max_length=MAX_LEN_SENTENCE, num_layers=num_layers, num_heads=num_heads)\n",
        "transf_dec = MyTransformerDecoder(num_tgt_tokens, input_dim=input_dim, hidden_key_dim=hidden_key_dim, hidden_val_dim=hidden_val_dim, output_dim=output_dim, enc_emb_dim=output_dim, max_length=MAX_CODE_LEN, num_layers=num_layers, num_heads=num_heads)\n",
        "model = MyTransformerTranslator(transf_enc, transf_dec, len(CODE_VOCABULARY))\n",
        "\n",
        "# Step 4: Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=200,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.001,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Split dataset into train and eval\n",
        "train_size = int(0.8 * len(transl_dataset))\n",
        "eval_size = len(transl_dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(transl_dataset, [train_size, eval_size])\n",
        "\n",
        "# Custom Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MGMACM7bVOnt",
        "outputId": "50f8bfb2-9e12-49f1-deb5-9a14503d9b73"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16000' max='16000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16000/16000 11:50, Epoch 200/200]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.493900</td>\n",
              "      <td>2.537457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.335000</td>\n",
              "      <td>2.285964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.125800</td>\n",
              "      <td>2.190226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.099200</td>\n",
              "      <td>2.112291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.188500</td>\n",
              "      <td>2.029186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.918700</td>\n",
              "      <td>1.968030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.822500</td>\n",
              "      <td>1.880706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.739800</td>\n",
              "      <td>1.793312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.647800</td>\n",
              "      <td>1.727037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.699300</td>\n",
              "      <td>1.654134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.559400</td>\n",
              "      <td>1.600123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.653400</td>\n",
              "      <td>1.575508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.471000</td>\n",
              "      <td>1.525362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.582500</td>\n",
              "      <td>1.497023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.425900</td>\n",
              "      <td>1.455350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.561300</td>\n",
              "      <td>1.423214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.362600</td>\n",
              "      <td>1.395818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.454700</td>\n",
              "      <td>1.347571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.301600</td>\n",
              "      <td>1.326310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.147400</td>\n",
              "      <td>1.293678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.271300</td>\n",
              "      <td>1.270509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.094300</td>\n",
              "      <td>1.244323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.526800</td>\n",
              "      <td>1.222820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.120900</td>\n",
              "      <td>1.201406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.248800</td>\n",
              "      <td>1.185294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.155600</td>\n",
              "      <td>1.169843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.353500</td>\n",
              "      <td>1.157343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.087300</td>\n",
              "      <td>1.141036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.037900</td>\n",
              "      <td>1.126324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.155100</td>\n",
              "      <td>1.114865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.180200</td>\n",
              "      <td>1.120582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.064300</td>\n",
              "      <td>1.086612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.033100</td>\n",
              "      <td>1.074348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.142800</td>\n",
              "      <td>1.074951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.109700</td>\n",
              "      <td>1.063706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.309900</td>\n",
              "      <td>1.051955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.111300</td>\n",
              "      <td>1.040235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.076900</td>\n",
              "      <td>1.028584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.358200</td>\n",
              "      <td>1.022123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.213000</td>\n",
              "      <td>1.014974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.248800</td>\n",
              "      <td>1.007038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.859900</td>\n",
              "      <td>0.998938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.877600</td>\n",
              "      <td>0.988238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.953100</td>\n",
              "      <td>0.984394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.230300</td>\n",
              "      <td>0.975189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.902800</td>\n",
              "      <td>0.969545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.922600</td>\n",
              "      <td>0.962166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.810200</td>\n",
              "      <td>0.961847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.831200</td>\n",
              "      <td>0.959074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.801000</td>\n",
              "      <td>0.948733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.793600</td>\n",
              "      <td>0.945699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.822200</td>\n",
              "      <td>0.937201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.056800</td>\n",
              "      <td>0.927630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.720900</td>\n",
              "      <td>0.930031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.741200</td>\n",
              "      <td>0.932095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.086700</td>\n",
              "      <td>0.917931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.933100</td>\n",
              "      <td>0.915419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.841200</td>\n",
              "      <td>0.902721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.704500</td>\n",
              "      <td>0.901785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.969600</td>\n",
              "      <td>0.902300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.880300</td>\n",
              "      <td>0.899577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.918800</td>\n",
              "      <td>0.896659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.874700</td>\n",
              "      <td>0.883435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.820800</td>\n",
              "      <td>0.885067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.042400</td>\n",
              "      <td>0.873644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.748800</td>\n",
              "      <td>0.873490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.633100</td>\n",
              "      <td>0.866696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.836600</td>\n",
              "      <td>0.860357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.841200</td>\n",
              "      <td>0.859348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.000400</td>\n",
              "      <td>0.866373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.754900</td>\n",
              "      <td>0.853709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.682300</td>\n",
              "      <td>0.847669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.069300</td>\n",
              "      <td>0.848636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.072500</td>\n",
              "      <td>0.850287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.015000</td>\n",
              "      <td>0.840573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.723200</td>\n",
              "      <td>0.837137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.782300</td>\n",
              "      <td>0.836461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.972600</td>\n",
              "      <td>0.822726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.829900</td>\n",
              "      <td>0.818031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.834000</td>\n",
              "      <td>0.817148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.117300</td>\n",
              "      <td>0.816572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.974500</td>\n",
              "      <td>0.807319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.823100</td>\n",
              "      <td>0.812423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.058500</td>\n",
              "      <td>0.808713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.318700</td>\n",
              "      <td>0.799623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.632800</td>\n",
              "      <td>0.793515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.798300</td>\n",
              "      <td>0.788720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.632500</td>\n",
              "      <td>0.791830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.585900</td>\n",
              "      <td>0.786588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.938200</td>\n",
              "      <td>0.784754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.883100</td>\n",
              "      <td>0.777931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>0.772730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.853500</td>\n",
              "      <td>0.774698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.682100</td>\n",
              "      <td>0.783974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.490200</td>\n",
              "      <td>0.777994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.664000</td>\n",
              "      <td>0.779548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.744900</td>\n",
              "      <td>0.770946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.883600</td>\n",
              "      <td>0.761778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.294800</td>\n",
              "      <td>0.763231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.687400</td>\n",
              "      <td>0.765381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.732100</td>\n",
              "      <td>0.759339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.242400</td>\n",
              "      <td>0.765279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.558000</td>\n",
              "      <td>0.762848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.742900</td>\n",
              "      <td>0.759610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.703000</td>\n",
              "      <td>0.751747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.337100</td>\n",
              "      <td>0.753575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.938400</td>\n",
              "      <td>0.749703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.488600</td>\n",
              "      <td>0.752646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.763500</td>\n",
              "      <td>0.746920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.790800</td>\n",
              "      <td>0.748137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.784600</td>\n",
              "      <td>0.746268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.423500</td>\n",
              "      <td>0.738776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.712200</td>\n",
              "      <td>0.748403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.816700</td>\n",
              "      <td>0.741086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.022700</td>\n",
              "      <td>0.746247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.572800</td>\n",
              "      <td>0.744530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.490900</td>\n",
              "      <td>0.745984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.915800</td>\n",
              "      <td>0.739302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.638800</td>\n",
              "      <td>0.752663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.897300</td>\n",
              "      <td>0.739504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.731000</td>\n",
              "      <td>0.742742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.571300</td>\n",
              "      <td>0.736086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.609200</td>\n",
              "      <td>0.738355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.541200</td>\n",
              "      <td>0.744688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.642800</td>\n",
              "      <td>0.735012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.413700</td>\n",
              "      <td>0.730768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>1.061200</td>\n",
              "      <td>0.733464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.584700</td>\n",
              "      <td>0.727838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.309200</td>\n",
              "      <td>0.738634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.675600</td>\n",
              "      <td>0.734658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.796500</td>\n",
              "      <td>0.735095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.396600</td>\n",
              "      <td>0.736985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.736500</td>\n",
              "      <td>0.738150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>1.104800</td>\n",
              "      <td>0.733157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.644000</td>\n",
              "      <td>0.738373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.819900</td>\n",
              "      <td>0.725013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.640900</td>\n",
              "      <td>0.736520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.714100</td>\n",
              "      <td>0.735187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.873900</td>\n",
              "      <td>0.729980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.568400</td>\n",
              "      <td>0.727128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.445800</td>\n",
              "      <td>0.744077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.363500</td>\n",
              "      <td>0.739216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.583500</td>\n",
              "      <td>0.729638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.351400</td>\n",
              "      <td>0.729898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.371500</td>\n",
              "      <td>0.731739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.633700</td>\n",
              "      <td>0.728346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.550300</td>\n",
              "      <td>0.727098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.593200</td>\n",
              "      <td>0.731883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.258600</td>\n",
              "      <td>0.737282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.521400</td>\n",
              "      <td>0.732213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.445900</td>\n",
              "      <td>0.737254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.317800</td>\n",
              "      <td>0.726276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.501200</td>\n",
              "      <td>0.733775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.648400</td>\n",
              "      <td>0.740062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.429300</td>\n",
              "      <td>0.732891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.497100</td>\n",
              "      <td>0.732541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.448000</td>\n",
              "      <td>0.726854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.380500</td>\n",
              "      <td>0.728080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.496200</td>\n",
              "      <td>0.728159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.900800</td>\n",
              "      <td>0.726481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.707500</td>\n",
              "      <td>0.726258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.835600</td>\n",
              "      <td>0.726802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.421200</td>\n",
              "      <td>0.729370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.477400</td>\n",
              "      <td>0.730157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.785700</td>\n",
              "      <td>0.729331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.320600</td>\n",
              "      <td>0.732632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.790900</td>\n",
              "      <td>0.729998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.769300</td>\n",
              "      <td>0.730358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.656600</td>\n",
              "      <td>0.730729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.574800</td>\n",
              "      <td>0.729146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.807300</td>\n",
              "      <td>0.725690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.507400</td>\n",
              "      <td>0.730720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.435000</td>\n",
              "      <td>0.734719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.572400</td>\n",
              "      <td>0.733526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.755200</td>\n",
              "      <td>0.733385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.803900</td>\n",
              "      <td>0.728411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.544600</td>\n",
              "      <td>0.733851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.415900</td>\n",
              "      <td>0.728447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.688200</td>\n",
              "      <td>0.729967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.286400</td>\n",
              "      <td>0.727452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.522600</td>\n",
              "      <td>0.728473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.535700</td>\n",
              "      <td>0.728367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.353500</td>\n",
              "      <td>0.730093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.422600</td>\n",
              "      <td>0.728509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.678400</td>\n",
              "      <td>0.728746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.737300</td>\n",
              "      <td>0.728908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.663100</td>\n",
              "      <td>0.732735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.388600</td>\n",
              "      <td>0.732354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.829500</td>\n",
              "      <td>0.730184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.368500</td>\n",
              "      <td>0.731786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.869800</td>\n",
              "      <td>0.731284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.263200</td>\n",
              "      <td>0.731882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.387200</td>\n",
              "      <td>0.731992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.533300</td>\n",
              "      <td>0.730752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.357400</td>\n",
              "      <td>0.730192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.530300</td>\n",
              "      <td>0.730257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.466600</td>\n",
              "      <td>0.730646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.493800</td>\n",
              "      <td>0.731002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>0.731146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.704300</td>\n",
              "      <td>0.731133</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=16000, training_loss=0.8446894781142473, metrics={'train_runtime': 710.8454, 'train_samples_per_second': 45.017, 'train_steps_per_second': 22.508, 'total_flos': 0.0, 'train_loss': 0.8446894781142473, 'epoch': 200.0})"
            ]
          },
          "metadata": {},
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_examples = 200\n",
        "data = generate_data(n_examples)\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZjq73ar0qWf",
        "outputId": "c27a1aad-549f-4c2b-c57f-95016c67c8bf"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['shape_list', 'code_str', 'text', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [d[\"text\"] for d in data]\n",
        "code_lists = [d[\"code_str\"] for d in data]"
      ],
      "metadata": {
        "id": "ia4ctxrX04LE"
      },
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = TranslationDataset(texts, code_lists, MyTokenizer(VOCABULARY), MyRandomMaskTokenizer(CODE_VOCABULARY), MAX_LEN_SENTENCE, tgt_max_len=MAX_CODE_LEN)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=2)\n",
        "total_correct = 0\n",
        "total_samples = len(texts)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in eval_loader:\n",
        "        src_emb = batch['source_embeddings']\n",
        "        tgt_emb = batch['target_embeddings']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        outputs = model(source_embeddings=src_emb, target_embeddings=tgt_emb, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=-1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHnXexw-WNG9",
        "outputId": "1aaae312-5606-477d-9fe9-723eabf04a5c"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 72.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text, src_tokenizer, tgt_tokenizer, src_max_length, tgt_max_length):\n",
        "    src_tokens = src_tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=src_max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "    translation = \"\"\n",
        "    tgt_tokens = tgt_tokenizer.encode_plus(\n",
        "            translation,\n",
        "            add_special_tokens=True,\n",
        "            max_length=tgt_max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "    token_count = 1\n",
        "    while True and token_count < tgt_max_length:\n",
        "      src_token_list = src_tokens[\"input_ids\"].unsqueeze(0)\n",
        "      tgt_token_list = tgt_tokens[\"input_ids\"].unsqueeze(0)\n",
        "      att_mask = torch.cat((torch.ones(token_count), torch.zeros(tgt_token_list.shape[1] - token_count)))\n",
        "      print(src_token_list)\n",
        "      print(tgt_token_list)\n",
        "      print(att_mask)\n",
        "      print(src_token_list.shape[1])\n",
        "      print(tgt_token_list.shape[1])\n",
        "      print(len(att_mask))\n",
        "      outputs = model(source_embeddings=src_tokens[\"input_ids\"], target_embeddings=tgt_tokens[\"input_ids\"], attention_mask=tgt_tokens[\"attention_mask\"])\n",
        "      _, pred = torch.max(output, dim=-1)\n",
        "      next_word = CODE_VOCABULARY[pred]\n",
        "      if next_word == EOS_TOKEN:\n",
        "        break\n",
        "      translation.append(next_word)\n",
        "    return \" \".join(translation)\n",
        "\n",
        "n_examples = 10\n",
        "data = generate_data(n_examples)\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sN-d39Qs_xV",
        "outputId": "322d7d90-f84e-4f71-f9de-4209cf367bb8"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['shape_list', 'code_str', 'text', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 333
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for d in data:\n",
        "  print(translate(d[\"text\"], MyTokenizer(VOCABULARY), MyRandomMaskTokenizer(CODE_VOCABULARY), MAX_LEN_SENTENCE, MAX_CODE_LEN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YsgL4TBL5aWq",
        "outputId": "86dee516-dcb3-4ce1-8bbe-99c4fa0651a1"
      },
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[10,  9,  3, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])\n",
            "tensor([[0, 4, 3, 3, 3, 3, 3, 3]])\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "16\n",
            "8\n",
            "8\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-334-992eea1b8519>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCABULARY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyRandomMaskTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCODE_VOCABULARY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_CODE_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-333-094e4909f3c9>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(text, src_tokenizer, tgt_tokenizer, src_max_length, tgt_max_length)\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_token_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mnext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCODE_VOCABULARY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-245-aa1c37b2465e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source_embeddings, target_embeddings, attention_mask, labels)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msource_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransf_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdecoded_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransf_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-228-b68e76ffe7a2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, enc_emb, attention_mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-227-bac158482de9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, enc_emb, attention_mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-218-7c8d1b23a5d3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, encoder_embeddings, attention_mask)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# We do this with the unsqueeze method, which adds a new dimension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mattnmats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_key_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouHaPl3T5goI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}